{"docstore/metadata": {"e96f6f8b-7d82-445c-8f69-3709abfde183": {"doc_hash": "9ad6e968a1ed806695ca1ca45cbc355e6c6c7bfd4b51492b1c6f3d3500508960"}, "e0ec18af-7044-4d71-8e1c-0c57075d1b0d": {"doc_hash": "a350f5c4f7b84300807b1db1305886e5a1050d71b77d073f3e8bfe6a08b9d300"}, "a40b57fa-e19c-4778-8152-de96ea340d2c": {"doc_hash": "070dd1d8674c721c836a60b5d955bcdd75825a5f0b40d3d9aa8def75d34a35cd"}, "a92888dd-3cf4-4d1b-85cc-8d69fb25ec66": {"doc_hash": "4009e392fe7e97c589c52a93f30775f3242b699225bf1eee5278a05a9392c528"}, "f1fb6a8a-bffb-4f5d-8a41-18b4789ed1e8": {"doc_hash": "5b7081205bd89d2824adf5a95ad908bf10497d237e3797e89185739c2a92bfed"}, "53ece806-c0ac-496d-b0c1-aba2fd9d5584": {"doc_hash": "8f72f15b9321c4e9ea0b5420e32c8b0efd1af769b586d3c6681b8b311a21229e"}, "03090305-db0a-49a6-8abd-fc340693a1db": {"doc_hash": "613f9d67a7a521f33369f65f2512fc5626a11f4accb56caf449e48147615fc2b"}, "fea356c1-9131-4f17-bd90-2bc2553ff672": {"doc_hash": "e3e0ce639c568ca085527d83f00422436ad5643bede039e78e8b6bed14d6314f"}, "727c9798-98de-46db-a290-c0d01aca189f": {"doc_hash": "8dbad8f0a3bc61a2979c39123f9ab44b5c233d4c4f6d7c3825b8e5baf2d92786"}, "70018603-c48e-4570-9b60-3a7346ec33df": {"doc_hash": "01b0b28ffb52a5a4cd2aab904fbcd105159fffcce308f81bc5bbb9cfc2187c0a"}, "d4ccbabb-52f3-4580-a827-e608498351e5": {"doc_hash": "e0a810dac88b4262d9623ed5515901bab11156b2ffafedc3243ee136941906ec"}, "3f493e24-fb48-404d-bce6-93f9ccd989ad": {"doc_hash": "50682e486092f0789d0d983606094db4dfbb6936cbb79f4b79b6037644ee6335"}, "cbde5101-5369-4482-865c-46111003cf86": {"doc_hash": "a7a9ebf93934bd7b7cc2a48040701c358599d87b17e69c13f9e2b27ee83d8a35"}, "756df0d3-df4b-411f-94e3-1af878c90ae9": {"doc_hash": "0c4dba51aa39396c525b10f7b6f9ef5f5a85060b39b1d15b115da21512813b04"}, "9f751dea-9c23-4922-9deb-1b796b769609": {"doc_hash": "445bb347edea61d76dac604776e29ed5e6e17bd17571dacbaf9e21bbc39960eb"}, "7cac0a20-b98d-4376-a7a2-eecc378e53c3": {"doc_hash": "0dd351a36036b634990f1efefc22f676a9a50cb39519c56ee675b71473ea0921"}, "337dc3d9-dc1c-4087-b50e-bed6e0fab618": {"doc_hash": "bc350194e959bc354d91cf8b6e0e7b01a95a7a65c300cf85d64b0a7c03c97be9"}, "1ac69fbe-54d7-465c-92d2-fd2fee280ce3": {"doc_hash": "5dd23d690a35bedf21e8d785dfd2a8695ff6b3b4a9adb472955061941afe7d1d"}, "1815f9be-960c-48ea-8f5c-790013fc6516": {"doc_hash": "5345ca57d480f38f2892c3af1f946aa13e3850912ba632489cd0cfa7d569e330"}, "b9f11827-92a1-4bf9-8481-9bb2e8276320": {"doc_hash": "4b390effbf8b3b3d9f67e44d4733b7face6f443f7a6b6e57b249efe95f05fa55"}, "8d8af2b0-6414-40a6-b5a5-0d5bc422e002": {"doc_hash": "84639504fccedb1b03a4b0717c1afc70f035c4a1ea6a22c414ef5c233ec98edc"}, "73cd2ddd-601f-43cc-b123-6f3d58f21df9": {"doc_hash": "5a53cbf5d1d33fe9005eff37e8983720d44255c263e987b2e799e7a3ca2ffb64"}, "ae03e0cd-6484-4f01-8ba6-1e8ceebe5b09": {"doc_hash": "67952238f18c532ed40d7dee986b5988ee5c2cdfc4fe124d063559af76c999c1"}, "f79e085a-e4ba-4c3e-8364-6d874eaa357a": {"doc_hash": "09d1eb46f70456affd15a359cef68a40e5455f6eb0f1bc59cba9179ce6ec2a1c"}, "06dde59c-1d62-4992-84f4-2d56e4115733": {"doc_hash": "7b04dc8b55354486eb211cd7072f3c0cc782aff8b39a6b9921bc192662845d9d"}, "75dd8a66-ad33-40a8-a872-06a9238ef1be": {"doc_hash": "8640f3d3c3b74927fe739e37de3eda61a4324018f2532c8cba4ee5ef7f2fd945"}, "ce1291ec-6f1b-4c66-aa69-8728baa54296": {"doc_hash": "1c60afd445fba272228b0f8b3c4ca31206a4666507ab01e6b49b050676dee9c9"}, "16287717-1851-44fb-9b74-fd4e34938784": {"doc_hash": "402a0856ef3736b0be6ac67da050f36bd1ec6eecde346dfd415e0c771bfe6a81"}, "40364f47-f234-4556-ab03-c812ef5fa704": {"doc_hash": "47c9a769ba63e92603b9d7f867808517d2f18b5e7ee0e96df29388eb48336951"}, "6a38b887-6a27-4fdb-9462-ad31875cc8c0": {"doc_hash": "61f33dce84ec8586910f91c90fc9e901b14be8a4dad4f6069048bcdaf6a4fc87"}, "f3aabd79-dc1d-4bc1-a2c2-3dc526c6ec10": {"doc_hash": "618c15b4fb2a496cf17da3f4f141858459564b4be6cf4adf4346d3fc018df1fb"}, "1da77baf-cafc-4db0-b96d-0fb377dcb6ae": {"doc_hash": "9ef2d4d06bba8a90f62336c4481ca93cf93857651d8d573dd2a8f24e875ab702"}, "490140c2-92ae-4ebc-8d1d-9ecf9bfa930d": {"doc_hash": "7d5a0be3d0aa64744e26d5fcab7daf12ef47f81d727a6a6fe599755a14660a0c"}, "e8258fff-dc68-4d2b-8de8-03f922fd5aea": {"doc_hash": "94130e12606285a9f0f6e7a95ed0f6f531f8b9c0183c9881a37ef08fbf83c81c"}, "77e9d094-8f80-42fe-b28b-53739391062c": {"doc_hash": "df39ce466a0e41ab537453af6538a2eb49fd6836ff88fa95ef03f1d2c902b80a"}, "ecd134f4-e9ee-41b4-8db5-d4466a56bc56": {"doc_hash": "d4113fb81a8cc7c62625e027d6bfa10cca46710fe1f42932ad3be4e5c2ab8b9e"}, "b9cad8a4-43d4-4d9e-8a63-8bdab5ef612a": {"doc_hash": "7e124e82ec0b942d7d01169785507ca6540e6c5550c30913d8a962b087173db2", "ref_doc_id": "e96f6f8b-7d82-445c-8f69-3709abfde183"}, "288068d9-ccea-44c3-b8d5-3d53f7c0d53c": {"doc_hash": "0f042784e720ad8b18bd2214b473cf56d239a6421f147c3a2a095fe27c4f6a17", "ref_doc_id": "e0ec18af-7044-4d71-8e1c-0c57075d1b0d"}, "493a97bc-d778-42a9-8746-544b4d50a35b": {"doc_hash": "412ab7e7fabc4572f747e5a6d1dd15e491e99b08dfea2b3680a31db2ee69ba58", "ref_doc_id": "a40b57fa-e19c-4778-8152-de96ea340d2c"}, "e6556128-f1d0-46fc-9ca4-bf0d7c971fab": {"doc_hash": "d89fd1038e5910fcb47d0f49c3c3971259062e30d745cefcad8e3b7eb4a63c99", "ref_doc_id": "a92888dd-3cf4-4d1b-85cc-8d69fb25ec66"}, "618c011b-1ce5-4e27-a5fc-2455a2e03ba0": {"doc_hash": "bce576b65cedd744508a8461d4f112a1f6e9f3a6d628d4b212db6b59ae4f67a3", "ref_doc_id": "f1fb6a8a-bffb-4f5d-8a41-18b4789ed1e8"}, "9d18bff4-9186-4099-9952-58b5551259b8": {"doc_hash": "9a2b40332150234e7e90a6e610e4346cc56732fa3dcb10bbb6039c45862c8e30", "ref_doc_id": "53ece806-c0ac-496d-b0c1-aba2fd9d5584"}, "e56df99d-4c1e-4a22-b5ba-4ec6710c59e0": {"doc_hash": "62739fda170ffd4526df8d5f405a4e3f6caa95528072b452590288cf051e2555", "ref_doc_id": "03090305-db0a-49a6-8abd-fc340693a1db"}, "a448586b-c9af-4e96-9f15-ff34dfeb5847": {"doc_hash": "551656ea4dc67924bc852d84fdd9dbfd60fdf33eb6a5ff80a0a4f7ea63f5af84", "ref_doc_id": "fea356c1-9131-4f17-bd90-2bc2553ff672"}, "dff7ed46-7222-48ae-8dbc-883f3fa40d4c": {"doc_hash": "73b39dd7570ef028814905d66599e168089371421c4932b6f694b9da77cfaa17", "ref_doc_id": "727c9798-98de-46db-a290-c0d01aca189f"}, "cadd7b48-c685-4b46-b459-4836ba6a8abc": {"doc_hash": "93932dbc499041ce9ba0f8457f683db2b8f9c2017a993324b419c65345f2a423", "ref_doc_id": "70018603-c48e-4570-9b60-3a7346ec33df"}, "4767d97b-867e-4955-a481-8323277bf8d3": {"doc_hash": "6e3b0c089097c1a9c29cc883a0d33770d59de75d2cef527488db61978533a85e", "ref_doc_id": "d4ccbabb-52f3-4580-a827-e608498351e5"}, "80758997-9f1b-4bca-82c9-03e6b2ee7573": {"doc_hash": "1957a500eacd4594aa16addee8156fdf48124261a3e17480e1b15e3cfd0237ed", "ref_doc_id": "3f493e24-fb48-404d-bce6-93f9ccd989ad"}, "11c93f64-82f8-493a-8629-50b50b9d3418": {"doc_hash": "a9cdce98c5aae6bf7aca2f8d04c2c29ec207c461c8a4426a552b3766aca9f9a1", "ref_doc_id": "cbde5101-5369-4482-865c-46111003cf86"}, "28a447d6-1557-4e5e-ab15-c95c313875f9": {"doc_hash": "a0016e995655d511657103d6bd6bc8b357795aa8350495d18568cccc30ec48b9", "ref_doc_id": "756df0d3-df4b-411f-94e3-1af878c90ae9"}, "58a94e53-e6c2-4ce0-bc16-67e614f0dd78": {"doc_hash": "25649ea328ac7dc54ac2ae6904a0bdb2864c69b497b9d03a80e297c0ea16ca4c", "ref_doc_id": "9f751dea-9c23-4922-9deb-1b796b769609"}, "91b665e9-1cac-4ff9-8338-27e2b3d9e9bd": {"doc_hash": "5be8225d5c7e8a84503c85b84ebec2490b356012efd0d82b412d5c99769d31e5", "ref_doc_id": "7cac0a20-b98d-4376-a7a2-eecc378e53c3"}, "6a5b5df5-5f3f-4826-bd06-ab41668e9cf1": {"doc_hash": "ed13be6f4b645c61fb19f0f6f0e172ce040690a936d472710454962f410e8557", "ref_doc_id": "337dc3d9-dc1c-4087-b50e-bed6e0fab618"}, "f14cab1b-e278-4979-814d-3327508f1542": {"doc_hash": "82fdbfdf4fd6bbed3c7b388937734609ac6bd94f8e7f6a38ca1df7afd6057ddc", "ref_doc_id": "337dc3d9-dc1c-4087-b50e-bed6e0fab618"}, "4c23b670-23a4-484c-9698-e366e6f68ed1": {"doc_hash": "d0ba0b1cbd99a1bcacec53ceb3b7dd5bf8cd3054980c3777109b03cdf827f8a5", "ref_doc_id": "1ac69fbe-54d7-465c-92d2-fd2fee280ce3"}, "c1fb69bf-82ff-4593-84e2-7b6a5545408c": {"doc_hash": "45a35e85034d07c19f71b602f30a9c167ab66e11d71c6bd0def621e629fe035c", "ref_doc_id": "1ac69fbe-54d7-465c-92d2-fd2fee280ce3"}, "2977affb-138a-44d2-8c73-23ba3fdd1a25": {"doc_hash": "3df2d372599ed22664f2e4203330b5d48d07ed32d8b971507a1cedfdb54e47d4", "ref_doc_id": "1815f9be-960c-48ea-8f5c-790013fc6516"}, "6cd3667d-de22-4b44-9159-99a8f73712de": {"doc_hash": "ea19dec2f7aba73d4dee7c88966f1e255f92c8557738a8c5006d4956cf81862b", "ref_doc_id": "1815f9be-960c-48ea-8f5c-790013fc6516"}, "afa700a0-cc75-4678-b80d-d3a76c40a78a": {"doc_hash": "39eb615c313f08e90381153a0faffc734b696ff66b2ea952bc8ca6b018d47369", "ref_doc_id": "b9f11827-92a1-4bf9-8481-9bb2e8276320"}, "bb2b22ac-d07c-4a1e-aabd-d412050a6f46": {"doc_hash": "59edd0a10674af93a12fe3cd7d7bc791ad364c2604cb104b6176c1dcb091d2a7", "ref_doc_id": "b9f11827-92a1-4bf9-8481-9bb2e8276320"}, "8d2c7fee-73d6-4fd9-9d31-e35c1116bf74": {"doc_hash": "ebb1651abaf7381d7a867626927460dd25c551fdcbe080dcfac354cccabc0158", "ref_doc_id": "8d8af2b0-6414-40a6-b5a5-0d5bc422e002"}, "97c5a966-cf49-4ac8-be31-fb4a105f9472": {"doc_hash": "69c6c2416cb24aaa8d6c34fd4db3f75ba63812341927a1baf504bb94d4f343a6", "ref_doc_id": "73cd2ddd-601f-43cc-b123-6f3d58f21df9"}, "d37074ae-af0d-4880-872e-db300881db35": {"doc_hash": "775cc090466eb4b0b55408422586c40bd4d9cc93d0d80fa5fec92dd3a01a26eb", "ref_doc_id": "ae03e0cd-6484-4f01-8ba6-1e8ceebe5b09"}, "c01521d1-ae3f-4321-9d9f-1030f1080c2b": {"doc_hash": "3c420cca9801e187c6ccce140cce3150c5513a36c40d65060734a320b8f9f237", "ref_doc_id": "f79e085a-e4ba-4c3e-8364-6d874eaa357a"}, "84fa3ab5-e358-4ddb-ac8b-6953e7a3db47": {"doc_hash": "2afb12c5401b46fa7cbf3ecab225c14f724cf5e3efb720cb4ec9aae8ee22cb7f", "ref_doc_id": "06dde59c-1d62-4992-84f4-2d56e4115733"}, "a2828e88-65e6-4051-96b1-8aee312e38aa": {"doc_hash": "99883f913f39c37e002956990d8ff5ff1dbed6e02355dc3a110edae451691bea", "ref_doc_id": "06dde59c-1d62-4992-84f4-2d56e4115733"}, "3bf48011-656f-4851-aa36-f5eab3d37d77": {"doc_hash": "02267b37fd1316ab49d9e36919d52e2c9b3a0f94bc70f7860b090ad1aab28bf0", "ref_doc_id": "75dd8a66-ad33-40a8-a872-06a9238ef1be"}, "49c96674-a0d5-49a8-b790-b39101aa00cb": {"doc_hash": "d0a707e47cd1877542c870bce6411801ffea15f7f56b9cfe69034fb3bc4ace3b", "ref_doc_id": "ce1291ec-6f1b-4c66-aa69-8728baa54296"}, "6a72e312-5571-4322-b5a9-979f57fa83de": {"doc_hash": "560a8cbe099af2a5c10cdb1878751a270194d28c7d22bf1219c5cbaf93c398bb", "ref_doc_id": "16287717-1851-44fb-9b74-fd4e34938784"}, "2d2dfcc9-a505-4d2e-9503-80ee8d75615d": {"doc_hash": "8fb83cc173b6e082a597c7a43a28073eb56aa816e1fcf7ee1a79e3da9af09e38", "ref_doc_id": "40364f47-f234-4556-ab03-c812ef5fa704"}, "513dbdb1-d10a-499a-a1a7-2a78c6fef113": {"doc_hash": "94352a6f919fc15a3b89863f4a6cfeb133a517a8f4af6d343188cc340c16feb0", "ref_doc_id": "6a38b887-6a27-4fdb-9462-ad31875cc8c0"}, "f2e1306b-c5b1-484c-a3bf-5b63f80b5525": {"doc_hash": "807ec7fb6da2f068ad2f73087cdc98dc7c29b89c7daab44d72eaafadd31c7cb8", "ref_doc_id": "f3aabd79-dc1d-4bc1-a2c2-3dc526c6ec10"}, "a858936b-9acf-4c3c-bd1f-2c635f7907ce": {"doc_hash": "775521cc36345634822c011625319743d5b9c8701f1daa8f7e41e0d7faef8ec4", "ref_doc_id": "1da77baf-cafc-4db0-b96d-0fb377dcb6ae"}, "4430a9e9-56b2-416f-ab75-56a800f803ef": {"doc_hash": "ad1e7e71fbbc325205b16a14f05f363b840c8b255cdba12d1a31fe00a25d240a", "ref_doc_id": "490140c2-92ae-4ebc-8d1d-9ecf9bfa930d"}, "e1b44fb9-f03d-4fc7-a90f-2f3e465bde5a": {"doc_hash": "540d6a4ca7bd92401b53bdf7cf3318d44b30731a77b2eb43866248140b387df1", "ref_doc_id": "e8258fff-dc68-4d2b-8de8-03f922fd5aea"}, "1c4b5a02-82a9-42a3-b653-a9cc1c608bfb": {"doc_hash": "d99e7951d41be6f8f90ae992eaa8984d7c68dc2d1423429c3e396ee95d52eba6", "ref_doc_id": "77e9d094-8f80-42fe-b28b-53739391062c"}, "60991f2d-f3a6-4739-a4d5-d4574b79afc4": {"doc_hash": "50253bf681a0ef3ae6decfacfcae7fad730b0ca5dd07e46c8f5088fb8df87028", "ref_doc_id": "ecd134f4-e9ee-41b4-8db5-d4466a56bc56"}}, "docstore/data": {"b9cad8a4-43d4-4d9e-8a63-8bdab5ef612a": {"__data__": {"id_": "b9cad8a4-43d4-4d9e-8a63-8bdab5ef612a", "embedding": null, "metadata": {"page_label": "1", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e96f6f8b-7d82-445c-8f69-3709abfde183", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "9ad6e968a1ed806695ca1ca45cbc355e6c6c7bfd4b51492b1c6f3d3500508960", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "288068d9-ccea-44c3-b8d5-3d53f7c0d53c", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "0f042784e720ad8b18bd2214b473cf56d239a6421f147c3a2a095fe27c4f6a17", "class_name": "RelatedNodeInfo"}}, "hash": "7e124e82ec0b942d7d01169785507ca6540e6c5550c30913d8a962b087173db2", "text": "Training Compute-Optimal Large Language Models\nJordan Ho\ufb00mann\u2605, Sebastian Borgeaud\u2605, Arthur Mensch\u2605, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\nErich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre\u2605\n\u2605Equal contributions\nWeinvestigatetheoptimalmodelsizeandnumberoftokensfortrainingatransformerlanguagemodel\nunder a given compute budget. We \ufb01nd that current large language models are signi\ufb01cantly under-\ntrained, a consequence of the recent focus on scaling language models whilst keeping the amount of\ntrainingdataconstant. Bytrainingover400languagemodelsrangingfrom70milliontoover16billion\nparameters on 5 to 500 billion tokens, we \ufb01nd that for compute-optimal training, the model size and\nthe number of training tokens should be scaled equally: for every doubling of model size the number\nof training tokens should also be doubled. We test this hypothesis by training a predicted compute-\noptimal model, Chinchilla , that uses the same compute budget as Gopherbut with 70B parameters and\n4\u0002more more data. Chinchilla uniformly and signi\ufb01cantly outperforms Gopher(280B), GPT-3 (175B),\nJurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.\nThis also means that Chinchilla uses substantially less compute for \ufb01ne-tuning and inference, greatly\nfacilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of\n67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.\n1. Introduction\nRecently a series of Large Language Models (LLMs) have been introduced (Brown et al., 2020; Lieber\net al., 2021; Rae et al., 2021; Smith et al., 2022; Thoppilan et al., 2022), with the largest dense\nlanguage models now having over 500 billion parameters. These large autoregressive transformers\n(Vaswani et al., 2017) have demonstrated impressive performance on many tasks using a variety of\nevaluation protocols such as zero-shot, few-shot, and \ufb01ne-tuning.\nThe compute and energy cost for training large language models is substantial (Rae et al., 2021;\nThoppilan et al., 2022) and rises with increasing model size. In practice, the allocated training\ncompute budget is often known in advance: how many accelerators are available and for how long\nwe want to use them. Since it is typically only feasible to train these large models once, accurately\nestimating the best model hyperparameters for a given compute budget is critical (Tay et al., 2021).\nKaplan et al. (2020) showed that there is a power law relationship between the number of\nparameters in an autoregressive language model (LM) and its performance. As a result, the \ufb01eld has\nbeentraininglargerandlargermodels,expectingperformanceimprovements. Onenotableconclusion\nin Kaplan et al. (2020) is that large models should not be trained to their lowest possible loss to be\ncompute optimal. Whilst we reach the same conclusion, we estimate that large models should be\ntrained for many more training tokens than recommended by the authors. Speci\ufb01cally, given a 10\u0002\nincrease computational budget, they suggests that the size of the model should increase 5\u00935\u0002while\nthe number of training tokens should only increase 1.8 \u0002. Instead, we \ufb01nd that model size and the\nnumber of training tokens should be scaled in equal proportions.\nFollowing Kaplan et al. (2020) and the training setup of GPT-3 (Brown et al., 2020), many of the\nrecently trained large models have been trained for approximately 300 billion tokens (Table 1), in\nline with the approach of predominantly increasing model size when increasing compute.\nCorresponding authors: {jordanho\ufb00mann|sborgeaud|amensch|sifre}@deepmind.com\n\u00a92023 DeepMind. All rights reservedarXiv:2203.15556v1  [cs.CL]  29 Mar 2022", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "288068d9-ccea-44c3-b8d5-3d53f7c0d53c": {"__data__": {"id_": "288068d9-ccea-44c3-b8d5-3d53f7c0d53c", "embedding": null, "metadata": {"page_label": "2", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0ec18af-7044-4d71-8e1c-0c57075d1b0d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "a350f5c4f7b84300807b1db1305886e5a1050d71b77d073f3e8bfe6a08b9d300", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9cad8a4-43d4-4d9e-8a63-8bdab5ef612a", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "7e124e82ec0b942d7d01169785507ca6540e6c5550c30913d8a962b087173db2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "493a97bc-d778-42a9-8746-544b4d50a35b", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "412ab7e7fabc4572f747e5a6d1dd15e491e99b08dfea2b3680a31db2ee69ba58", "class_name": "RelatedNodeInfo"}}, "hash": "0f042784e720ad8b18bd2214b473cf56d239a6421f147c3a2a095fe27c4f6a17", "text": "10171019102110231025\nFLOPs10M100M1.0B10B100B1TParameters\nApproach 1\nApproach 2\nApproach 3\nKaplan et al (2020)\n \nChinchilla (70B)\nGopher (280B)\nGPT-3 (175B)\nMegatron-Turing NLG (530B)Figure 1jOverlaid predictions. We overlay the predictions from our three di\ufb00erent approaches,\nalong with projections from Kaplan et al. (2020). We \ufb01nd that all three methods predict that current\nlarge models should be substantially smaller and therefore trained much longer than is currently\ndone. In Figure A3, we show the results with the predicted optimal tokens plotted against the optimal\nnumberofparametersfor\ufb01xedFLOPbudgets. Chinchilla outperforms Gopherandtheotherlarge\nmodels (see Section 4.2).\nIn this work, we revisit the question: Given a \ufb01xed FLOPs budget,1how should one trade-o\ufb00 model\nsize and the number of training tokens? To answer this question, we model the \ufb01nal pre-training loss2\n\ud835\udc3f\u00b9\ud835\udc41\u0094\ud835\udc37\u00baas a function of the number of model parameters \ud835\udc41, and the number of training tokens, \ud835\udc37.\nSince the computational budget \ud835\udc36is a deterministic function FLOPs\u00b9\ud835\udc41\u0094\ud835\udc37\u00baof the number of seen\ntraining tokens and model parameters, we are interested in minimizing \ud835\udc3funder the constraint\nFLOPs\u00b9\ud835\udc41\u0094\ud835\udc37\u00ba=\ud835\udc36:\n\ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61\u00b9\ud835\udc36\u00ba\u0094\ud835\udc37\ud835\udc5c\ud835\udc5d\ud835\udc61\u00b9\ud835\udc36\u00ba= argmin\n\ud835\udc41\u0094\ud835\udc37s.t. FLOPs\u00b9\ud835\udc41\u0094\ud835\udc37\u00ba=\ud835\udc36\ud835\udc3f\u00b9\ud835\udc41\u0094\ud835\udc37\u00ba\u0093 (1)\nThe functions \ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61\u00b9\ud835\udc36\u00ba, and\ud835\udc37\ud835\udc5c\ud835\udc5d\ud835\udc61\u00b9\ud835\udc36\u00badescribe the optimal allocation of a computational budget \ud835\udc36. We\nempirically estimate these functions based on the losses of over 400 models, ranging from under 70M\nto over 16B parameters, and trained on 5B to over 400B tokens \u2013 with each model con\ufb01guration\ntrained for several di\ufb00erent training horizons. Our approach leads to considerably di\ufb00erent results\nthan that of Kaplan et al. (2020). We highlight our results in Figure 1 and how our approaches di\ufb00er\nin Section 2.\nBased on our estimated compute-optimal frontier, we predict that for the compute budget used\nto trainGopher, an optimal model should be 4 times smaller, while being training on 4 times more\ntokens. We verify thisby training a more compute-optimal 70B model, called Chinchilla , on 1.4 trillion\ntokens. Not only does Chinchilla outperform its much larger counterpart, Gopher, but its reduced\nmodel size reduces inference cost considerably and greatly facilitates downstream uses on smaller\nhardware. The energy cost of a large language model is amortized through its usage for inference an\n\ufb01ne-tuning. The bene\ufb01ts of a more optimally trained smaller model, therefore, extend beyond the\nimmediate bene\ufb01ts of its improved performance.\n1For example, knowing the number of accelerators and a target training duration.\n2For simplicity, we perform our analysis on the smoothed training loss which is an unbiased estimate of the test loss, as\nwe are in the in\ufb01nite data regime (the number of training tokens is less than the number of tokens in the entire corpus).\n2", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "493a97bc-d778-42a9-8746-544b4d50a35b": {"__data__": {"id_": "493a97bc-d778-42a9-8746-544b4d50a35b", "embedding": null, "metadata": {"page_label": "3", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a40b57fa-e19c-4778-8152-de96ea340d2c", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "070dd1d8674c721c836a60b5d955bcdd75825a5f0b40d3d9aa8def75d34a35cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "288068d9-ccea-44c3-b8d5-3d53f7c0d53c", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "0f042784e720ad8b18bd2214b473cf56d239a6421f147c3a2a095fe27c4f6a17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6556128-f1d0-46fc-9ca4-bf0d7c971fab", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "d89fd1038e5910fcb47d0f49c3c3971259062e30d745cefcad8e3b7eb4a63c99", "class_name": "RelatedNodeInfo"}}, "hash": "412ab7e7fabc4572f747e5a6d1dd15e491e99b08dfea2b3680a31db2ee69ba58", "text": "Table 1jCurrent LLMs . We show \ufb01ve of the current largest dense transformer models, their size,\nand the number of training tokens. Other than LaMDA (Thoppilan et al., 2022), most models are\ntrained for approximately 300 billion tokens. We introduce Chinchilla , a substantially smaller model,\ntrained for much longer than 300B tokens.\nModel Size ( #Parameters) Training Tokens\nLaMDA (Thoppilan et al., 2022) 137 Billion 168 Billion\nGPT-3 (Brown et al., 2020) 175 Billion 300 Billion\nJurassic (Lieber et al., 2021) 178 Billion 300 Billion\nGopher(Rae et al., 2021) 280 Billion 300 Billion\nMT-NLG 530B (Smith et al., 2022) 530 Billion 270 Billion\nChinchilla 70 Billion 1.4 Trillion\n2. Related Work\nLarge language models. A variety of large language models have been introduced in the last few\nyears. These include both dense transformer models (Brown et al., 2020; Lieber et al., 2021; Rae\net al., 2021; Smith et al., 2022; Thoppilan et al., 2022) and mixture-of-expert (MoE) models (Du\net al., 2021; Fedus et al., 2021; Zoph et al., 2022). The largest dense transformers have passed 500\nbillion parameters (Smith et al., 2022). The drive to train larger and larger models is clear\u2014so far\nincreasingthesizeoflanguagemodelshasbeenresponsibleforimprovingthestate-of-the-artinmany\nlanguage modelling tasks. Nonetheless, large language models face several challenges, including\ntheir overwhelming computational requirements (the cost of training and inference increase with\nmodel size) (Rae et al., 2021; Thoppilan et al., 2022) and the need for acquiring more high-quality\ntraining data. In fact, in this work we \ufb01nd that larger, high quality datasets will play a key role in any\nfurther scaling of language models.\nModelling the scaling behavior. Understanding the scaling behaviour of language models and\ntheir transfer properties has been important in the development of recent large models (Hernandez\netal.,2021;Kaplanetal.,2020). Kaplanetal.(2020)\ufb01rstshowedapredictablerelationshipbetween\nmodel size and loss over many orders of magnitude. The authors investigate the question of choosing\nthe optimal model size to train for a given compute budget. Similar to us, they address this question\nby training various models. Our work di\ufb00ers from Kaplan et al. (2020) in several important ways.\nFirst, the authors use a \ufb01xed number of training tokens and learning rate schedule for all models; this\nprevents them from modelling the impact of these hyperparameters on the loss. In contrast, we \ufb01nd\nthat setting the learning rate schedule to approximately match the number of training tokens results\nin the best \ufb01nal loss regardless of model size\u2014see Figure A1. For a \ufb01xed learning rate cosine schedule\nto 130B tokens, the intermediate loss estimates (for \ud835\udc370\u009d\u009d130B) are therefore overestimates of the\nloss of a model trained with a schedule length matching \ud835\udc370. Using these intermediate losses results in\nunderestimating the e\ufb00ectiveness of training models on less data than 130B tokens, and eventually\ncontributestotheconclusionthatmodelsizeshouldincreasefasterthantrainingdatasizeascompute\nbudget increases. In contrast, our analysis predicts that both quantities should scale at roughly the\nsame rate. Secondly, we include models with up to 16B parameters, as we observe that there is slight\ncurvature in the FLOP-loss frontier (see Appendix E)\u2014in fact, the majority of the models used in\nour analysis have more than 500 million parameters, in contrast the majority of runs in Kaplan et al.\n(2020) are signi\ufb01cantly smaller\u2014many being less than 100M parameters.\nRecently, Clark et al. (2022) speci\ufb01cally looked in to the scaling properties of Mixture of Expert\n3", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6556128-f1d0-46fc-9ca4-bf0d7c971fab": {"__data__": {"id_": "e6556128-f1d0-46fc-9ca4-bf0d7c971fab", "embedding": null, "metadata": {"page_label": "4", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a92888dd-3cf4-4d1b-85cc-8d69fb25ec66", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "4009e392fe7e97c589c52a93f30775f3242b699225bf1eee5278a05a9392c528", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "493a97bc-d778-42a9-8746-544b4d50a35b", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "412ab7e7fabc4572f747e5a6d1dd15e491e99b08dfea2b3680a31db2ee69ba58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "618c011b-1ce5-4e27-a5fc-2455a2e03ba0", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "bce576b65cedd744508a8461d4f112a1f6e9f3a6d628d4b212db6b59ae4f67a3", "class_name": "RelatedNodeInfo"}}, "hash": "d89fd1038e5910fcb47d0f49c3c3971259062e30d745cefcad8e3b7eb4a63c99", "text": "language models, showing that the scaling with number of experts diminishes as the model size\nincreases\u2014their approach models the loss as a function of two variables: the model size and the\nnumber of experts. However, the analysis is done with a \ufb01xed number of training tokens, as in Kaplan\net al. (2020), potentially underestimating the improvements of branching.\nEstimatinghyperparametersforlargemodels. Themodelsizeandthenumberoftrainingtokens\nare not the only two parameters to chose when selecting a language model and a procedure to train\nit. Other important factors include learning rate, learning rate schedule, batch size, optimiser, and\nwidth-to-depth ratio. In this work, we focus on model size and the number of training steps, and\nwe rely on existing work and provided experimental heuristics to determine the other necessary\nhyperparameters. Yang et al. (2021) investigates how to choose a variety of these parameters for\ntraining an autoregressive transformer, including the learning rate and batch size. McCandlish et al.\n(2018) \ufb01nds only a weak dependence between optimal batch size and model size. Shallue et al.\n(2018);Zhangetal.(2019)suggestthatusinglargerbatch-sizesthanthoseweuseispossible. Levine\net al. (2020) investigates the optimal depth-to-width ratio for a variety of standard model sizes. We\nuse slightly less deep models than proposed as this translates to better wall-clock performance on our\nhardware.\nImproved model architectures. Recently, various promising alternatives to traditional dense trans-\nformers have been proposed. For example, through the use of conditional computation large MoE\nmodels like the 1.7 trillion parameter Switch transformer (Fedus et al., 2021), the 1.2 Trillion pa-\nrameter GLaM model (Du et al., 2021), and others (Artetxe et al., 2021; Zoph et al., 2022) are able\nto provide a large e\ufb00ective model size despite using relatively fewer training and inference FLOPs.\nHowever, for very large models the computational bene\ufb01ts of routed models seems to diminish (Clark\net al., 2022). An orthogonal approach to improving language models is to augment transformers\nwith explicit retrieval mechanisms, as done by Borgeaud et al. (2021); Guu et al. (2020); Lewis et al.\n(2020). This approach e\ufb00ectively increases the number of data tokens seen during training (by a\nfactor of\u001810in Borgeaud et al. (2021)). This suggests that the performance of language models\nmay be more dependant on the size of the training data than previously thought.\n3. Estimating the optimal parameter/training tokens allocation\nWe present three di\ufb00erent approaches to answer the question driving our research: Given a \ufb01xed\nFLOPs budget, how should one trade-o\ufb00 model size and the number of training tokens? In all three\ncases we start by training a range of models varying both model size and the number of training\ntokens and use the resulting training curves to \ufb01t an empirical estimator of how they should scale.\nWe assume a power-law relationship between compute and model size as done in Clark et al. (2022);\nKaplan et al. (2020), though future work may want to include potential curvature in this relationship\nfor large model sizes. The resulting predictions are similar for all three methods and suggest that\nparameter count and number of training tokens should be increased equally with more compute3\u2014\nwith proportions reported in Table 2. This is in clear contrast to previous work on this topic and\nwarrants further investigation.\n3We compute FLOPs as described in Appendix F.\n4", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "618c011b-1ce5-4e27-a5fc-2455a2e03ba0": {"__data__": {"id_": "618c011b-1ce5-4e27-a5fc-2455a2e03ba0", "embedding": null, "metadata": {"page_label": "5", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1fb6a8a-bffb-4f5d-8a41-18b4789ed1e8", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "5b7081205bd89d2824adf5a95ad908bf10497d237e3797e89185739c2a92bfed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6556128-f1d0-46fc-9ca4-bf0d7c971fab", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "d89fd1038e5910fcb47d0f49c3c3971259062e30d745cefcad8e3b7eb4a63c99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d18bff4-9186-4099-9952-58b5551259b8", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "9a2b40332150234e7e90a6e610e4346cc56732fa3dcb10bbb6039c45862c8e30", "class_name": "RelatedNodeInfo"}}, "hash": "bce576b65cedd744508a8461d4f112a1f6e9f3a6d628d4b212db6b59ae4f67a3", "text": "101710181019102010211022\nFLOPS2.02.53.03.54.04.55.05.56.0Training loss\n75M250M500M1B2.5B5B10B\n10171019102110231025\nFLOPs109101010111012Tokens\n1.5T\n10171019102110231025\nFLOPs100M1.0B10B100B1TParameters\n67BFigure 2jTraining curve envelope. On theleftwe show all of our di\ufb00erent runs. We launched a\nrange of model sizes going from 70M to 10B, each for four di\ufb00erent cosine cycle lengths. From these\ncurves, we extracted the envelope of minimal loss per FLOP, and we used these points to estimate the\noptimal model size ( center) for a given compute budget and the optimal number of training tokens\n(right). In green, we show projections of optimal model size and training token count based on the\nnumber of FLOPs used to train Gopher(5\u009376\u00021023).\n3.1. Approach 1: Fix model sizes and vary number of training tokens\nIn our \ufb01rst approach we vary the number of training steps for a \ufb01xed family of models (ranging from\n70M to over 10B parameters), training each model for 4 di\ufb00erent number of training sequences.\nFrom these runs, we are able to directly extract an estimate of the minimum loss achieved for a given\nnumber of training FLOPs. Training details for this approach can be found in Appendix D.\nFor each parameter count \ud835\udc41we train 4 di\ufb00erent models, decaying the learning rate by a factor of\n10\u0002over a horizon (measured in number of training tokens) that ranges by a factor of 16\u0002. Then, for\neach run, we smooth and then interpolate the training loss curve. From this, we obtain a continuous\nmapping from FLOP count to training loss for each run. Then, for each FLOP count, we determine\nwhich run achieves the lowest loss. Using these interpolants, we obtain a mapping from any FLOP\ncount\ud835\udc36, to the most e\ufb03cient choice of model size \ud835\udc41and number of training tokens \ud835\udc37such that\nFLOPs\u00b9\ud835\udc41\u0094\ud835\udc37\u00ba=\ud835\udc36.4At1500logarithmicallyspacedFLOPvalues,we\ufb01ndwhichmodelsizeachievesthe\nlowest loss of all models along with the required number of training tokens. Finally, we \ufb01t power laws\nto estimate the optimal model size and number of training tokens for any given amount of compute\n(see the center and right panels of Figure 2), obtaining a relationship \ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61/\ud835\udc36\ud835\udc4eand\ud835\udc37\ud835\udc5c\ud835\udc5d\ud835\udc61/\ud835\udc36\ud835\udc4f. We\n\ufb01nd that\ud835\udc4e=0\u009350and\ud835\udc4f=0\u009350\u2014as summarized in Table 2. In Section D.4, we show a head-to-head\ncomparison at 1021FLOPs, using the model size recommended by our analysis and by the analysis of\nKaplan et al. (2020)\u2014using the model size we predict has a clear advantage.\n3.2. Approach 2: IsoFLOP pro\ufb01les\nIn our second approach we vary the model size5for a \ufb01xed set of 9 di\ufb00erent training FLOP counts6\n(ranging from 6\u00021018to3\u00021021FLOPs), and consider the \ufb01nal training loss for each point7. in\ncontrast with Approach 1 that considered points \u00b9\ud835\udc41\u0094\ud835\udc37\u0094\ud835\udc3f\u00baalong the entire training runs. This allows\nus to directly answer the question: For a given FLOP budget, what is the optimal parameter count?\n4Notethatallselectedpointsarewithinthelast15%oftraining. Thissuggeststhatwhentrainingamodelover \ud835\udc37tokens,\nwe should pick a cosine cycle length that decays 10\u0002over approximately \ud835\udc37tokens\u2014see further details in Appendix B.\n5In approach 2, model size varies up to 16B as opposed to approach 1 where we only used models up to 10B.\n6The number of training tokens is determined by the model size and training FLOPs.\n7We set the cosine schedule length to match the number of tokens, which is optimal according to the analysis presented\nin Appendix B.\n5", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d18bff4-9186-4099-9952-58b5551259b8": {"__data__": {"id_": "9d18bff4-9186-4099-9952-58b5551259b8", "embedding": null, "metadata": {"page_label": "6", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53ece806-c0ac-496d-b0c1-aba2fd9d5584", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "8f72f15b9321c4e9ea0b5420e32c8b0efd1af769b586d3c6681b8b311a21229e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "618c011b-1ce5-4e27-a5fc-2455a2e03ba0", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "bce576b65cedd744508a8461d4f112a1f6e9f3a6d628d4b212db6b59ae4f67a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e56df99d-4c1e-4a22-b5ba-4ec6710c59e0", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "62739fda170ffd4526df8d5f405a4e3f6caa95528072b452590288cf051e2555", "class_name": "RelatedNodeInfo"}}, "hash": "9a2b40332150234e7e90a6e610e4346cc56732fa3dcb10bbb6039c45862c8e30", "text": "100M 300M 1B 3B6B 30B\nParameters2.02.22.42.62.83.03.2Training Loss\n6e18\n1e19\n3e19\n6e19\n1e20\n3e20\n6e20\n1e21\n3e21\n10171019102110231025\nFLOPs100M1B10B100B1TParameters\n63B\n10171019102110231025\nFLOPs100M1B10B100B1T10TTokens\n1.4TFigure 3jIsoFLOP curves. For various model sizes, we choose the number of training tokens such\nthat the \ufb01nal FLOPs is a constant. The cosine cycle length is set to match the target FLOP count. We\n\ufb01nd a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train\n(left). Using the location of these valleys, we project optimal model size and number of tokens for\nlarger models ( centerandright). In green, we show the estimated number of parameters and tokens\nfor anoptimalmodel trained with the compute budget of Gopher.\nFor each FLOP budget, we plot the \ufb01nal loss (after smoothing) against the parameter count in\nFigure 3 (left). In all cases, we ensure that we have trained a diverse enough set of model sizes to see\na clear minimum in the loss. We \ufb01t a parabola to each IsoFLOPs curve to directly estimate at what\nmodel size the minimum loss is achieved (Figure 3 (left)). As with the previous approach, we then \ufb01t\na power law between FLOPs and loss-optimal model size and number of training tokens, shown in\nFigure 3 (center, right). Again, we \ufb01t exponents of the form \ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61/\ud835\udc36\ud835\udc4eand\ud835\udc37\ud835\udc5c\ud835\udc5d\ud835\udc61/\ud835\udc36\ud835\udc4fand we \ufb01nd that\n\ud835\udc4e=0\u009349and\ud835\udc4f=0\u009351\u2014as summarized in Table 2.\n3.3. Approach 3: Fitting a parametric loss function\nLastly, we model all \ufb01nal losses from experiments in Approach 1 & 2 as a parametric function of\nmodel parameter count and the number of seen tokens. Following a classical risk decomposition (see\nSection D.2), we propose the following functional form\n\u02c6\ud835\udc3f\u00b9\ud835\udc41\u0094\ud835\udc37\u00ba,\ud835\udc38\u00b8\ud835\udc34\n\ud835\udc41\ud835\udefc\u00b8\ud835\udc35\n\ud835\udc37\ud835\udefd\u0093 (2)\nThe \ufb01rst term captures the loss for an ideal generative process on the data distribution, and should\ncorrespond to the entropy of natural text. The second term captures the fact that a perfectly trained\ntransformer with \ud835\udc41parameters underperforms the ideal generative process. The \ufb01nal term captures\nthe fact that the transformer is not trained to convergence, as we only make a \ufb01nite number of\noptimisation steps, on a sample of the dataset distribution.\nModel \ufb01tting. To estimate\u00b9\ud835\udc34\u0094\ud835\udc35\u0094\ud835\udc38\u0094\ud835\udefc\u0094\ud835\udefd\u00ba, we minimize the Huber loss (Huber, 1964) between the\npredicted and observed log loss using the L-BFGS algorithm (Nocedal, 1980):\nmin\n\ud835\udc34\u0094\ud835\udc35\u0094\ud835\udc38\u0094\ud835\udefc\u0094\ud835\udefd\u2211\ufe01\nRuns\ud835\udc56Huber\ud835\udeff\u0010\nlog\u02c6\ud835\udc3f\u00b9\ud835\udc41\ud835\udc56\u0094\ud835\udc37\ud835\udc56\u00ba\u0000log\ud835\udc3f\ud835\udc56\u0011\n(3)\nWe account for possible local minima by selecting the best \ufb01t from a grid of initialisations. The Huber\nloss (\ud835\udeff=10\u00003) is robust to outliers, which we \ufb01nd important for good predictive performance over\nheld-out data points. Section D.2 details the \ufb01tting procedure and the loss decomposition.\n6", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e56df99d-4c1e-4a22-b5ba-4ec6710c59e0": {"__data__": {"id_": "e56df99d-4c1e-4a22-b5ba-4ec6710c59e0", "embedding": null, "metadata": {"page_label": "7", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "03090305-db0a-49a6-8abd-fc340693a1db", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "613f9d67a7a521f33369f65f2512fc5626a11f4accb56caf449e48147615fc2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d18bff4-9186-4099-9952-58b5551259b8", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "9a2b40332150234e7e90a6e610e4346cc56732fa3dcb10bbb6039c45862c8e30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a448586b-c9af-4e96-9f15-ff34dfeb5847", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "551656ea4dc67924bc852d84fdd9dbfd60fdf33eb6a5ff80a0a4f7ea63f5af84", "class_name": "RelatedNodeInfo"}}, "hash": "62739fda170ffd4526df8d5f405a4e3f6caa95528072b452590288cf051e2555", "text": "101810191020102110221023Gopher\nbudget\nTraining FLOPs100M1B10B40B100BModel sizeIsoLoss contours\nEfficient frontier\nEmpirical data\nIsoFLOPs slice2.003.004.005.00Loss\n100M 1B 10B 40B\nModel sizeIsoFLOPs slices\nTrain. FLOPs\n6e+18\n1e+19\n3e+19\n6e+19\n1e+20\n3e+20\n6e+20\n1e+21\n3e+21\nGopher\nFigure 4jParametric \ufb01t. We \ufb01t a parametric modelling of the loss \u02c6\ud835\udc3f\u00b9\ud835\udc41\u0094\ud835\udc37\u00baand display contour ( left)\nand isoFLOP slices ( right). For each isoFLOP slice, we include a corresponding dashed line in the left\nplot. Intheleftplot, weshowthee\ufb03cientfrontierinblue, whichisalineinlog-logspace. Speci\ufb01cally,\nthe curve goes through each iso-loss contour at the point with the fewest FLOPs. We project the\noptimal model size given the GopherFLOP budget to be 40B parameters.\nE\ufb03cient frontier. We can approximate the functions \ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61and\ud835\udc37\ud835\udc5c\ud835\udc5d\ud835\udc61by minimizing the parametric\nloss\u02c6\ud835\udc3funder the constraint FLOPs\u00b9\ud835\udc41\u0094\ud835\udc37\u00ba\u00196\ud835\udc41\ud835\udc37(Kaplan et al., 2020). The resulting \ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61and\ud835\udc37\ud835\udc5c\ud835\udc5d\ud835\udc61\nbalance the two terms in Equation (3)that depend on model size and data. By construction, they\nhave a power-law form:\n\ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61\u00b9\ud835\udc36\u00ba=\ud835\udc3a\u0012\ud835\udc36\n6\u0013\ud835\udc4e\n\u0094 \ud835\udc37\ud835\udc5c\ud835\udc5d\ud835\udc61\u00b9\ud835\udc36\u00ba=\ud835\udc3a\u00001\u0012\ud835\udc36\n6\u0013\ud835\udc4f\n\u0094where\ud835\udc3a=\u0012\ud835\udefc\ud835\udc34\n\ud835\udefd\ud835\udc35\u00131\n\ud835\udefc\u00b8\ud835\udefd\n\u0094 \ud835\udc4e=\ud835\udefd\n\ud835\udefc\u00b8\ud835\udefd\u0094and\ud835\udc4f=\ud835\udefc\n\ud835\udefc\u00b8\ud835\udefd\u0093(4)\nWeshowcontoursofthe\ufb01ttedfunction \u02c6\ud835\udc3finFigure4(left),andtheclosed-forme\ufb03cientcomputational\nfrontier in blue. From this approach, we \ufb01nd that \ud835\udc4e=0\u009346and\ud835\udc4f=0\u009354\u2014as summarized in Table 2.\n3.4. Optimal model scaling\nWe \ufb01nd that the three approaches, despite using di\ufb00erent \ufb01tting methodologies and di\ufb00erent trained\nmodels, yield comparable predictions for the optimal scaling in parameters and tokens with FLOPs\n(shown in Table 2). All three approaches suggest that as compute budget increases, model size and\nthe amount of training data should be increased in approximately equal proportions. The \ufb01rst and\nsecond approaches yield very similar predictions for optimal model sizes, as shown in Figure 1 and\nFigure A3. The third approach predicts even smaller models being optimal at larger compute budgets.\nWe note that the observed points \u00b9\ud835\udc3f\u0094\ud835\udc41\u0094\ud835\udc37\u00bafor low training FLOPs ( \ud835\udc3661\ud835\udc5221) have larger residuals\nk\ud835\udc3f\u0000\u02c6\ud835\udc3f\u00b9\ud835\udc41\u0094\ud835\udc37\u00bak2\n2than points with higher computational budgets. The \ufb01tted model places increased\nweight on the points with more FLOPs\u2014automatically considering the low-computational budget\npoints as outliers due to the Huber loss. As a consequence of the empirically observed negative\ncurvature in the frontier \ud835\udc36!\ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61(see Appendix E), this results in predicting a lower \ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61than the\ntwo other approaches.\nIn Table 3 we show the estimated number of FLOPs and tokens that would ensure that a model of\na given size lies on the compute-optimal frontier. Our \ufb01ndings suggests that the current generation of\n7", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a448586b-c9af-4e96-9f15-ff34dfeb5847": {"__data__": {"id_": "a448586b-c9af-4e96-9f15-ff34dfeb5847", "embedding": null, "metadata": {"page_label": "8", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fea356c1-9131-4f17-bd90-2bc2553ff672", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "e3e0ce639c568ca085527d83f00422436ad5643bede039e78e8b6bed14d6314f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e56df99d-4c1e-4a22-b5ba-4ec6710c59e0", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "62739fda170ffd4526df8d5f405a4e3f6caa95528072b452590288cf051e2555", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dff7ed46-7222-48ae-8dbc-883f3fa40d4c", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "73b39dd7570ef028814905d66599e168089371421c4932b6f694b9da77cfaa17", "class_name": "RelatedNodeInfo"}}, "hash": "551656ea4dc67924bc852d84fdd9dbfd60fdf33eb6a5ff80a0a4f7ea63f5af84", "text": "Table 2jEstimated parameter and data scaling with increased training compute. The listed\nvalues are the exponents, \ud835\udc4eand\ud835\udc4f, on the relationship \ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61/\ud835\udc36\ud835\udc4eand\ud835\udc37\ud835\udc5c\ud835\udc5d\ud835\udc61/\ud835\udc36\ud835\udc4f. Our analysis suggests\na near equal scaling in parameters and data with increasing compute which is in clear contrast\nto previous work on the scaling of large models. The 10thand 90thpercentiles are estimated via\nbootstrapping data (80% of the dataset is sampled 100 times) and are shown in parenthesis.\nApproach Coe\ufb00. \ud835\udc4ewhere\ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61/\ud835\udc36\ud835\udc4eCoe\ufb00.\ud835\udc4fwhere\ud835\udc37\ud835\udc5c\ud835\udc5d\ud835\udc61/\ud835\udc36\ud835\udc4f\n1. Minimum over training curves 0\u009350\u00b90\u0093488\u00940\u0093502\u00ba 0\u009350\u00b90\u0093501\u00940\u0093512\u00ba\n2. IsoFLOP pro\ufb01les 0\u009349\u00b90\u0093462\u00940\u0093534\u00ba 0\u009351\u00b90\u0093483\u00940\u0093529\u00ba\n3. Parametric modelling of the loss 0\u009346\u00b90\u0093454\u00940\u0093455\u00ba 0\u009354\u00b90\u0093542\u00940\u0093543\u00ba\nKaplan et al. (2020) 0.73 0.27\nTable 3jEstimated optimal training FLOPs and training tokens for various model sizes. For\nvarious model sizes, we show the projections from Approach 1 of how many FLOPs and training\ntokens would be needed to train compute-optimal models. The estimates for Approach 2 & 3 are\nsimilar (shown in Section D.3)\n.Parameters FLOPs FLOPs (in Gopherunit) Tokens\n400 Million 1.92e+19 1\u009d29\u0094968 8.0 Billion\n1 Billion 1.21e+20 1\u009d4\u009476120.2 Billion\n10 Billion 1.23e+22 1\u009d46205.1 Billion\n67 Billion 5.76e+23 11.5 Trillion\n175 Billion 3.85e+24 6\u009373.7 Trillion\n280 Billion 9.90e+24 17\u009325.9 Trillion\n520 Billion 3.43e+25 59\u0093511.0 Trillion\n1 Trillion 1.27e+26 221\u0093321.2 Trillion\n10 Trillion 1.30e+28 22515\u00939216.2 Trillion\nlarge language models are considerably over-sized, given their respective compute budgets, as shown\ninFigure1. Forexample,we\ufb01ndthata175billionparametermodelshouldbetrainedwithacompute\nbudget of 4\u009341\u00021024FLOPs and on over 4.2 trillion tokens. A 280 billion Gopher-like model is the\noptimalmodeltotraingivenacomputebudgetofapproximately 1025FLOPsandshouldbetrainedon\n6.8 trillion tokens. Unless one has a compute budget of 1026FLOPs (over 250\u0002the compute used to\ntrainGopher), a 1 trillion parameter model is unlikely to be the optimal model to train. Furthermore,\nthe amount of training data that is projected to be needed is far beyond what is currently used to\ntrain large models, and underscores the importance of dataset collection in addition to engineering\nimprovements that allow for model scale. While there is signi\ufb01cant uncertainty extrapolating out\nmany orders of magnitude, our analysis clearly suggests that given the training compute budget for\nmany current LLMs, smaller models should have been trained on more tokens to achieve the most\nperformant model.\nIn Appendix C, we reproduce the IsoFLOP analysis on two additional datasets: C4 (Ra\ufb00el et al.,\n2020a) and GitHub code (Rae et al., 2021). In both cases we reach the similar conclusion that model\nsize and number of training tokens should be scaled in equal proportions.\n8", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dff7ed46-7222-48ae-8dbc-883f3fa40d4c": {"__data__": {"id_": "dff7ed46-7222-48ae-8dbc-883f3fa40d4c", "embedding": null, "metadata": {"page_label": "9", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "727c9798-98de-46db-a290-c0d01aca189f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "8dbad8f0a3bc61a2979c39123f9ab44b5c233d4c4f6d7c3825b8e5baf2d92786", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a448586b-c9af-4e96-9f15-ff34dfeb5847", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "551656ea4dc67924bc852d84fdd9dbfd60fdf33eb6a5ff80a0a4f7ea63f5af84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cadd7b48-c685-4b46-b459-4836ba6a8abc", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "93932dbc499041ce9ba0f8457f683db2b8f9c2017a993324b419c65345f2a423", "class_name": "RelatedNodeInfo"}}, "hash": "73b39dd7570ef028814905d66599e168089371421c4932b6f694b9da77cfaa17", "text": "4.Chinchilla\nBasedonouranalysisinSection3,theoptimalmodelsizeforthe Gophercomputebudgetissomewhere\nbetween 40 and 70 billion parameters. We test this hypothesis by training a model on the larger end\nof this range\u201470B parameters\u2014for 1.4T tokens, due to both dataset and computational e\ufb03ciency\nconsiderations. In this section we compare this model, which we call Chinchilla , toGopherand other\nLLMs. Both Chinchilla andGopherhave been trained for the same number of FLOPs but di\ufb00er in the\nsize of the model and the number of training tokens.\nWhile pre-training a large language model has a considerable compute cost, downstream \ufb01ne-\ntuning and inference also make up substantial compute usage (Rae et al., 2021). Due to being 4\u0002\nsmaller than Gopher, both the memory footprint and inference cost of Chinchilla are also smaller.\n4.1. Model and training details\nThe full set of hyperparameters used to train Chinchilla are given in Table 4. Chinchilla uses the same\nmodel architecture and training setup as Gopherwith the exception of the di\ufb00erences listed below.\n\u2022We train Chinchilla onMassiveText (the same dataset as Gopher) but use a slightly di\ufb00erent\nsubset distribution (shown in Table A1) to account for the increased number of training tokens.\n\u2022We use AdamW (Loshchilov and Hutter, 2019) for Chinchilla rather than Adam (Kingma and\nBa, 2014) as this improves the language modelling loss and the downstream task performance\nafter \ufb01netuning.8\n\u2022We train Chinchilla with a slightly modi\ufb01ed SentencePiece (Kudo and Richardson, 2018)\ntokenizer that does not apply NFKC normalisation. The vocabulary is very similar\u2013 94.15% of\ntokens are the same as those used for training Gopher. We \ufb01nd that this particularly helps with\nthe representation of mathematics and chemistry, for example.\n\u2022Whilst the forward and backward pass are computed in bfloat16 , we store a float32 copy\nof the weights in the distributed optimiser state (Rajbhandari et al., 2020). See Lessons Learned\nfrom Rae et al. (2021) for additional details.\nIn Appendix G we show the impact of the various optimiser related changes between Chinchilla\nandGopher. All models in this analysis have been trained on TPUv3/TPUv4 (Jouppi et al., 2017) with\nJAX (Bradbury et al., 2018) and Haiku (Hennigan et al., 2020). We include a Chinchilla model card\n(Mitchell et al., 2019) in Table A8.\nModel Layers Number Heads Key/Value Size d modelMax LR Batch Size\nGopher280B 80 128 128 16,384 4\u000210\u000053M!6M\nChinchilla 70B 80 64 128 8,192 1\u000210\u000041.5M!3M\nTable 4jChinchilla architecture details. We list the number of layers, the key/value size, the\nbottleneck activation size d model, the maximum learning rate, and the training batch size (# tokens).\nThe feed-forward size is always set to 4\u0002dmodel. Note that we double the batch size midway through\ntraining for both Chinchilla andGopher.\n8Interestingly,amodeltrainedwithAdamWonlypassesthetrainingperformanceofamodeltrainedwithAdamaround\n80% of the way through the cosine cycle, though the ending performance is notably better\u2013 see Figure A7\n9", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cadd7b48-c685-4b46-b459-4836ba6a8abc": {"__data__": {"id_": "cadd7b48-c685-4b46-b459-4836ba6a8abc", "embedding": null, "metadata": {"page_label": "10", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "70018603-c48e-4570-9b60-3a7346ec33df", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "01b0b28ffb52a5a4cd2aab904fbcd105159fffcce308f81bc5bbb9cfc2187c0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dff7ed46-7222-48ae-8dbc-883f3fa40d4c", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "73b39dd7570ef028814905d66599e168089371421c4932b6f694b9da77cfaa17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4767d97b-867e-4955-a481-8323277bf8d3", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "6e3b0c089097c1a9c29cc883a0d33770d59de75d2cef527488db61978533a85e", "class_name": "RelatedNodeInfo"}}, "hash": "93932dbc499041ce9ba0f8457f683db2b8f9c2017a993324b419c65345f2a423", "text": "# Tasks Examples\nLanguage Modelling 20 WikiText-103, The Pile: PG-19, arXiv, FreeLaw, \u0093\u0093\u0093\nReading Comprehension 3 RACE-m, RACE-h, LAMBADA\nQuestion Answering 3 Natural Questions, TriviaQA, TruthfulQA\nCommon Sense 5 HellaSwag, Winogrande, PIQA, SIQA, BoolQ\nMMLU 57 High School Chemistry, Astronomy, Clinical Knowledge, \u0093\u0093\u0093\nBIG-bench 62 Causal Judgement, Epistemic Reasoning, Temporal Sequences, \u0093\u0093\u0093\nTable 5jAll evaluation tasks. We evaluate Chinchilla on a collection of language modelling along\nwith downstream tasks. We evaluate on largely the same tasks as in Rae et al. (2021), to allow for\ndirect comparison.\n4.2. Results\nWe perform an extensive evaluation of Chinchilla , comparing against various large language models.\nWe evaluate on a large subset of the tasks presented in Rae et al. (2021), shown in Table 5. As\nthe focus of this work is on optimal model scaling, we included a large representative subset, and\nintroduce a few new evaluations to allow for better comparison to other existing large models. The\nevaluation details for all tasks are the same as described in Rae et al. (2021).\n4.2.1. Language modelling\npubmed_abstracts\nnih_exporter\nuspto_backgrounds\npubmed_central\npile_cc\nbookcorpus2\nstackexchange\nopensubtitles\nopenwebtext2\nhackernews\ndm_mathematics\narxiv\nfreelaw\nbooks3\nphilpapers\ngithub\nubuntu_irc\neuroparl\ngutenberg_pg_190.000.020.040.060.080.10Decrease in bpb \n compared to Gopher\nFigure 5jPile Evaluation. For the di\ufb00erent evaluation sets in The Pile (Gao et al., 2020), we show\nthe bits-per-byte (bpb) improvement (decrease) of Chinchilla compared to Gopher. On all subsets,\nChinchilla outperforms Gopher.\nChinchilla signi\ufb01cantly outperforms Gopheron all evaluation subsets of The Pile (Gao et al.,\n2020), as shown in Figure 5. Compared to Jurassic-1 (178B) Lieber et al. (2021), Chinchilla is more\nperformant on all but two subsets\u2013 dm_mathematics andubuntu_irc \u2013 see Table A5 for a raw\nbits-per-byte comparison. On Wikitext103 (Merity et al., 2017), Chinchilla achieves a perplexity of\n7.16 compared to 7.75 for Gopher. Some caution is needed when comparing Chinchilla withGopher\non these language modelling benchmarks as Chinchilla is trained on 4\u0002more data than Gopherand\nthus train/test set leakage may arti\ufb01cially enhance the results. We thus place more emphasis on other\n10", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4767d97b-867e-4955-a481-8323277bf8d3": {"__data__": {"id_": "4767d97b-867e-4955-a481-8323277bf8d3", "embedding": null, "metadata": {"page_label": "11", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4ccbabb-52f3-4580-a827-e608498351e5", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "e0a810dac88b4262d9623ed5515901bab11156b2ffafedc3243ee136941906ec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cadd7b48-c685-4b46-b459-4836ba6a8abc", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "93932dbc499041ce9ba0f8457f683db2b8f9c2017a993324b419c65345f2a423", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80758997-9f1b-4bca-82c9-03e6b2ee7573", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "1957a500eacd4594aa16addee8156fdf48124261a3e17480e1b15e3cfd0237ed", "class_name": "RelatedNodeInfo"}}, "hash": "6e3b0c089097c1a9c29cc883a0d33770d59de75d2cef527488db61978533a85e", "text": "Random 25.0%\nAverage human rater 34.5%\nGPT-3 5-shot 43.9%\nGopher5-shot 60.0%\nChinchilla 5-shot 67.6%\nAverage human expert performance 89.8%\nJune 2022 Forecast 57.1%\nJune 2023 Forecast 63.4%\nTable 6jMassive Multitask Language Understanding (MMLU). We report the average 5-shot\naccuracy over 57 tasks with model and human accuracy comparisons taken from Hendrycks et al.\n(2020). We also include the average prediction for state of the art accuracy in June 2022/2023 made\nby 73 competitive human forecasters in Steinhardt (2021).\ntasks for which leakage is less of a concern, such as MMLU (Hendrycks et al., 2020) and BIG-bench\n(BIG-bench collaboration, 2021) along with various closed-book question answering and common\nsense analyses.\n4.2.2. MMLU\nTheMassiveMultitaskLanguageUnderstanding(MMLU)benchmark(Hendrycksetal.,2020)consists\nof a range of exam-like questions on academic subjects. In Table 6, we report Chinchilla \u2019s average\n5-shotperformanceonMMLU(thefullbreakdownofresultsisshowninTableA6). Onthisbenchmark,\nChinchilla signi\ufb01cantly outperforms Gopherdespite being much smaller, with an average accuracy of\n67.6%(improvingupon Gopherby7.6%). Remarkably, Chinchilla evenoutperformstheexpertforecast\nfor June 2023 of 63.4% accuracy (see Table 6) (Steinhardt, 2021). Furthermore, Chinchilla achieves\ngreater than 90% accuracy on 4 di\ufb00erent individual tasks\u2013 high_school_gov_and_politics,\ninternational_law, sociology ,and us_foreign_policy . Toourknowledge,noothermodel\nhas achieved greater than 90% accuracy on a subset.\nIn Figure 6, we show a comparison to Gopherbroken down by task. Overall, we \ufb01nd that Chin-\nchillaimproves performance on the vast majority of tasks. On four tasks ( college_mathematics,\neconometrics, moral_scenarios , and formal_logic )Chinchilla underperforms Gopher, and\nthere is no change in performance on two tasks.\n4.2.3. Reading comprehension\nOn the \ufb01nal word prediction dataset LAMBADA (Paperno et al., 2016), Chinchilla achieves 77.4%\naccuracy, compared to 74.5% accuracy from Gopherand 76.6% from MT-NLG 530B (see Table 7). On\nRACE-h and RACE-m (Lai et al., 2017), Chinchilla greatly outperforms Gopher, improving accuracy\nby more than 10% in both cases\u2014see Table 7.\n4.2.4. BIG-bench\nWe analysed Chinchilla on the same set of BIG-bench tasks (BIG-bench collaboration, 2021) reported\nin Rae et al. (2021). Similar to what we observed in MMLU, Chinchilla outperforms Gopheron the\nvast majority of tasks (see Figure 7). We \ufb01nd that Chinchilla improves the average performance\nby 10.7%, reaching an accuracy of 65.1% versus 54.4% for Gopher. Of the 62 tasks we consider,\nChinchilla performsworsethan Gopherononlyfour\u2014 crash_blossom, dark_humor_detection,\n11", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80758997-9f1b-4bca-82c9-03e6b2ee7573": {"__data__": {"id_": "80758997-9f1b-4bca-82c9-03e6b2ee7573", "embedding": null, "metadata": {"page_label": "12", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f493e24-fb48-404d-bce6-93f9ccd989ad", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "50682e486092f0789d0d983606094db4dfbb6936cbb79f4b79b6037644ee6335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4767d97b-867e-4955-a481-8323277bf8d3", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "6e3b0c089097c1a9c29cc883a0d33770d59de75d2cef527488db61978533a85e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11c93f64-82f8-493a-8629-50b50b9d3418", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "a9cdce98c5aae6bf7aca2f8d04c2c29ec207c461c8a4426a552b3766aca9f9a1", "class_name": "RelatedNodeInfo"}}, "hash": "1957a500eacd4594aa16addee8156fdf48124261a3e17480e1b15e3cfd0237ed", "text": "college_mathematics\neconometrics\nmoral_scenarios\nformal_logic\nmedical_genetics\nmachine_learning\npublic_relations\nglobal_facts\nbusiness_ethics\nelectrical_engineering\ncollege_computer_science\nworld_religions\nhigh_school_us_history\nhigh_school_psychology\nmanagement\nhigh_school_computer_science\nmarketing\nhigh_school_physics\nhigh_school_macroeconomics\nsociology\nhigh_school_government_and_politics\nhigh_school_european_history\nnutrition\ncollege_medicine\nastronomy\nlogical_fallacies\nprofessional_psychology\nmiscellaneous\njurisprudence\nclinical_knowledge\nhigh_school_geography\nhigh_school_biology\ncollege_biology\ncollege_chemistry\nhigh_school_world_history\nus_foreign_policy\nvirology\nphilosophy\nmoral_disputes\nhuman_aging\ncomputer_security\nsecurity_studies\ninternational_law\nhigh_school_microeconomics\nhigh_school_statistics\nprofessional_accounting\nprofessional_medicine\nprehistory\nhigh_school_chemistry\nelementary_mathematics\nabstract_algebra\nanatomy\nprofessional_law\nhuman_sexuality\ncollege_physics\nhigh_school_mathematics\nconceptual_physics10\n0102030Relative Improvement \n over GopherFigure 6jMMLU results compared to GopherWe \ufb01nd that Chinchilla outperforms Gopherby 7.6%\non average (see Table 6) in addition to performing better on 51/57 individual tasks, the same on\n2/57, and worse on only 4/57 tasks.\nChinchilla Gopher GPT-3 MT-NLG 530B\nLAMBADA Zero-Shot 77.4 74.5 76.2 76.6\nRACE-m Few-Shot 86.8 75.1 58.1 -\nRACE-h Few-Shot 82.3 71.6 46.8 47.9\nTable7jReadingcomprehension. OnRACE-handRACE-m(Laietal.,2017), Chinchilla considerably\nimproves performance over Gopher. Note that GPT-3 and MT-NLG 530B use a di\ufb00erent prompt format\nthan we do on RACE-h/m, so results are not comparable to GopherandChinchilla . On LAMBADA\n(Paperno et al., 2016), Chinchilla outperforms both Gopherand MT-NLG 530B.\nmathematical_induction andlogical_args . Full accuracy results for Chinchilla can be found\nin Table A7.\n4.2.5. Common sense\nWe evaluate Chinchilla on various common sense benchmarks: PIQA (Bisk et al., 2020), SIQA (Sap\net al., 2019), Winogrande (Sakaguchi et al., 2020), HellaSwag (Zellers et al., 2019), and BoolQ\n(Clark et al., 2019). We \ufb01nd that Chinchilla outperforms both Gopherand GPT-3 on all tasks and\noutperforms MT-NLG 530B on all but one task\u2014see Table 8.\nOn TruthfulQA (Lin et al., 2021), Chinchilla reaches 43.6%, 58.5%, and 66.7% accuracy with\n0-shot,5-shot,and10-shotrespectively. Incomparison, Gopherachievedonly29.5%0-shotand43.7%\n10-shot accuracy. In stark contrast with the \ufb01ndings of Lin et al. (2021), the large improvements\n(14.1% in 0-shot accuracy) achieved by Chinchilla suggest that better modelling of the pre-training\ndata alone can lead to substantial improvements on this benchmark.\n12", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11c93f64-82f8-493a-8629-50b50b9d3418": {"__data__": {"id_": "11c93f64-82f8-493a-8629-50b50b9d3418", "embedding": null, "metadata": {"page_label": "13", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cbde5101-5369-4482-865c-46111003cf86", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "a7a9ebf93934bd7b7cc2a48040701c358599d87b17e69c13f9e2b27ee83d8a35", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80758997-9f1b-4bca-82c9-03e6b2ee7573", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "1957a500eacd4594aa16addee8156fdf48124261a3e17480e1b15e3cfd0237ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28a447d6-1557-4e5e-ab15-c95c313875f9", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "a0016e995655d511657103d6bd6bc8b357795aa8350495d18568cccc30ec48b9", "class_name": "RelatedNodeInfo"}}, "hash": "a9cdce98c5aae6bf7aca2f8d04c2c29ec207c461c8a4426a552b3766aca9f9a1", "text": "crash_blossom\ndark_humor_detection\nmathematical_induction\nlogical_args\ngeneral_knowledge_json\nHuman_organs_senses_multiple_choice\nformal_fallacies_syllogisms_negation\nknown_unknowns\nnavigate\nsentence_ambiguity\nmoral_permissibility\nintent_recognition\nirony_identification\nentailed_polarity\nhyperbaton\nmisconceptions\nevaluating_information_essentiality\nsimilarities_abstraction\nepistemic_reasoning\nfantasy_reasoning\nmovie_dialog_same_or_different\nwinowhy\nnovel_concepts\ndiscourse_marker_prediction\nstrategyqa\ncausal_judgment\nhindu_knowledge\nphrase_relatedness\nalignment_questionnaire\nreasoning_about_colored_objects\ndate_understanding\npenguins_in_a_table\nfigure_of_speech_detection\ndisambiguation_q\nimplicatures\nSNARKS\nruin_names\nlogical_fallacy_detection\nanachronisms\nlogic_grid_puzzle\nriddle_sense\nanalytic_entailment\nquestion_selection\nnonsense_words_grammar\nphysics_mc\nempirical_judgments\nsports_understanding\ncrass_ai\nphysical_intuition\ntimedial\nimplicit_relations\nenglish_proverbs\npresuppositions_as_nli\nmovie_recommendation\nunderstanding_fables\nmetaphor_boolean\ntemporal_sequences\nlogical_sequence\nidentify_odd_metaphor\ngre_reading_comprehension\nodd_one_out\nanalogical_similarity20\n020406080100120Relative Improvement \n over GopherFigure 7jBIG-bench results compared to GopherChinchilla out performs Gopheron all but four\nBIG-bench tasks considered. Full results are in Table A7.\n4.2.6. Closed-book question answering\nResults on closed-book question answering benchmarks are reported in Table 9. On the Natural\nQuestions dataset (Kwiatkowski et al., 2019), Chinchilla achieves new closed-book SOTA accuracies:\n31.5% 5-shot and 35.5% 64-shot, compared to 21% and 28% respectively, for Gopher. On TriviaQA\n(Joshi et al., 2017) we show results for both the \ufb01ltered (previously used in retrieval and open-book\nwork) and un\ufb01ltered set (previously used in large language model evaluations). In both cases,\nChinchilla substantially out performs Gopher. On the \ufb01ltered version, Chinchilla lags behind the open\nbook SOTA (Izacard and Grave, 2020) by only 7.9%. On the un\ufb01ltered set, Chinchilla outperforms\nGPT-3\u2014see Table 9.\n4.2.7. Gender bias and toxicity\nLarge Language Models carry potential risks such as outputting o\ufb00ensive language, propagating\nsocial biases, and leaking private information (Bender et al., 2021; Weidinger et al., 2021). We\nexpectChinchilla to carry risks similar to GopherbecauseChinchilla is trained on the same data,\nChinchilla Gopher GPT-3 MT-NLG 530B Supervised SOTA\nHellaSWAG 80.8% 79.2% 78.9% 80.2% 93.9%\nPIQA 81.8% 81.8% 81.0% 82.0% 90.1%\nWinogrande 74.9% 70.1% 70.2% 73.0% 91.3%\nSIQA 51.3% 50.6% - - 83.2%\nBoolQ 83.7% 79.3% 60.5% 78.2% 91.4%\nTable 8jZero-shot comparison on Common Sense benchmarks. We show a comparison between\nChinchilla ,Gopher, and MT-NLG 530B on various Common Sense benchmarks. We see that Chinchilla\nmatches or outperforms Gopherand GPT-3 on all tasks. On all but one Chinchilla outperforms the\nmuch larger MT-NLG 530B model.\n13", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28a447d6-1557-4e5e-ab15-c95c313875f9": {"__data__": {"id_": "28a447d6-1557-4e5e-ab15-c95c313875f9", "embedding": null, "metadata": {"page_label": "14", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "756df0d3-df4b-411f-94e3-1af878c90ae9", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "0c4dba51aa39396c525b10f7b6f9ef5f5a85060b39b1d15b115da21512813b04", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11c93f64-82f8-493a-8629-50b50b9d3418", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "a9cdce98c5aae6bf7aca2f8d04c2c29ec207c461c8a4426a552b3766aca9f9a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58a94e53-e6c2-4ce0-bc16-67e614f0dd78", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "25649ea328ac7dc54ac2ae6904a0bdb2864c69b497b9d03a80e297c0ea16ca4c", "class_name": "RelatedNodeInfo"}}, "hash": "a0016e995655d511657103d6bd6bc8b357795aa8350495d18568cccc30ec48b9", "text": "Method Chinchilla Gopher GPT-3 SOTA (open book)\nNatural Questions (dev)0-shot 16.6% 10.1% 14.6%\n54.4% 5-shot 31.5% 24.5% -\n64-shot 35.5% 28.2% 29.9%\nTriviaQA (un\ufb01ltered, test)0-shot 67.0% 52.8% 64.3 %\n- 5-shot 73.2% 63.6% -\n64-shot 72.3% 61.3% 71.2%\nTriviaQA (\ufb01ltered, dev)0-shot 55.4% 43.5% -\n72.5% 5-shot 64.1% 57.0% -\n64-shot 64.6% 57.2% -\nTable 9jClosed-book question answering. For Natural Questions (Kwiatkowski et al., 2019) and\nTriviaQA (Joshi et al., 2017), Chinchilla outperforms Gopherin all cases. On Natural Questions,\nChinchilla outperforms GPT-3. On TriviaQA we show results on two di\ufb00erent evaluation sets to allow\nfor comparison to GPT-3 and to open book SOTA (FiD + Distillation (Izacard and Grave, 2020)).\nalbeit with slightly di\ufb00erent relative weights, and because it has a similar architecture. Here, we\nexamine gender bias (particularly gender and occupation bias) and generation of toxic language. We\nselect a few common evaluations to highlight potential issues, but stress that our evaluations are not\ncomprehensive and much work remains to understand, evaluate, and mitigate risks in LLMs.\nGender bias. As discussed in Rae et al. (2021), large language models re\ufb02ect contemporary and\nhistorical discourse about di\ufb00erent groups (such as gender groups) from their training dataset, and\nwe expect the same to be true for Chinchilla . Here, we test if potential gender and occupation biases\nmanifest in unfair outcomes on coreference resolutions, using the Winogender dataset (Rudinger\net al., 2018) in a zero-shot setting. Winogender tests whether a model can correctly determine if\na pronoun refers to di\ufb00erent occupation words. An unbiased model would correctly predict which\nword the pronoun refers to regardless of pronoun gender. We follow the same setup as in Rae et al.\n(2021) (described further in Section H.3).\nAs shown in Table 10, Chinchilla correctly resolves pronouns more frequently than Gopheracross\nallgroups. Interestingly, theperformanceincreaseisconsiderablysmallerformalepronouns(increase\nof 3.2%) than for female or neutral pronouns (increases of 8.3% and 9.2% respectively). We also\nconsider gotchaexamples, in which the correct pronoun resolution contradicts gender stereotypes\n(determined by labor statistics). Again, we see that Chinchilla resolves pronouns more accurately\nthanGopher. When breaking up examples by male/female gender and gotcha/not gotcha , the largest\nimprovementisonfemale gotchaexamples(improvementof10%). Thus,though Chinchilla uniformly\novercomes gender stereotypes for more coreference examples than Gopher, the rate of improvement\nishigherforsomepronounsthanothers, suggestingthattheimprovementsconferredbyusingamore\ncompute-optimal model can be uneven.\nSample toxicity. Language models are capable of generating toxic language\u2014including insults,\nhate speech, profanities and threats (Gehman et al., 2020; Rae et al., 2021). While toxicity is an\numbrella term, and its evaluation in LMs comes with challenges (Welbl et al., 2021; Xu et al., 2021),\nautomatic classi\ufb01er scores can provide an indication for the levels of harmful text that a LM generates.\nRae et al. (2021) found that improving language modelling loss by increasing the number of model\nparameters has only a negligible e\ufb00ect on toxic text generation (unprompted); here we analyze\n14", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58a94e53-e6c2-4ce0-bc16-67e614f0dd78": {"__data__": {"id_": "58a94e53-e6c2-4ce0-bc16-67e614f0dd78", "embedding": null, "metadata": {"page_label": "15", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f751dea-9c23-4922-9deb-1b796b769609", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "445bb347edea61d76dac604776e29ed5e6e17bd17571dacbaf9e21bbc39960eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28a447d6-1557-4e5e-ab15-c95c313875f9", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "a0016e995655d511657103d6bd6bc8b357795aa8350495d18568cccc30ec48b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91b665e9-1cac-4ff9-8338-27e2b3d9e9bd", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "5be8225d5c7e8a84503c85b84ebec2490b356012efd0d82b412d5c99769d31e5", "class_name": "RelatedNodeInfo"}}, "hash": "25649ea328ac7dc54ac2ae6904a0bdb2864c69b497b9d03a80e297c0ea16ca4c", "text": "Chinchilla Gopher\nAll 78.3% 71.4%\nMale 71.2% 68.0%\nFemale 79.6% 71.3%\nNeutral 84.2% 75.0%Chinchilla Gopher\nMalegotcha 62.5% 59.2%\nMalenot gotcha 80.0% 76.7%\nFemalegotcha 76.7% 66.7%\nFemalenot gotcha 82.5% 75.8%\nTable 10jWinogender results. Left: Chinchilla consistently resolves pronouns better than Gopher.\nRight:Chinchilla performsbetteronexampleswhichcontradictgenderstereotypes( gotchaexamples).\nHowever, di\ufb00erence in performance across groups suggests Chinchilla exhibits bias.\nwhether the same holds true for a lower LM loss achieved via more compute-optimal training. Similar\nto the protocol of Rae et al. (2021), we generate 25,000 unprompted samples from Chinchilla , and\ncompare their PerspectiveAPI toxicity score distribution to that of Gopher-generated samples. Several\nsummary statistics indicate an absence of major di\ufb00erences: the mean (median) toxicity score for\nGopheris 0.081 (0.064), compared to 0.087 (0.066) for Chinchilla , and the 95thpercentile scores\nare 0.230 for Gopher, compared to 0.238 for Chinchilla . That is, the large majority of generated\nsamples are classi\ufb01ed as non-toxic, and the di\ufb00erence between the models is negligible. In line with\nprior \ufb01ndings (Rae et al., 2021), this suggests that toxicity levels in unconditional text generation\nare largely independent of the model quality (measured in language modelling loss), i.e. that better\nmodels of the training dataset are not necessarily more toxic.\n5. Discussion & Conclusion\nThe trend so far in large language model training has been to increase the model size, often without\nincreasing the number of training tokens. The largest dense transformer, MT-NLG 530B, is now\nover 3\u0002larger than GPT-3\u2019s 170 billion parameters from just two years ago. However, this model,\nas well as the majority of existing large models, have all been trained for a comparable number\nof tokens\u2014around 300 billion. While the desire to train these mega-models has led to substantial\nengineering innovation, we hypothesize that the race to train larger and larger models is resulting in\nmodels that are substantially underperforming compared to what could be achieved with the same\ncompute budget.\nWe propose three predictive approaches towards optimally setting model size and training dura-\ntion, based on the outcome of over 400 training runs. All three approaches predict that Gopheris\nsubstantially over-sized and estimate that for the same compute budget a smaller model trained on\nmoredatawillperformbetter. Wedirectlytestthishypothesisbytraining Chinchilla , a70Bparameter\nmodel, and show that it outperforms Gopherand even larger models on nearly every measured\nevaluation task.\nWhilst our method allows us to make predictions on how to scale large models when given\nadditional compute, there are several limitations. Due to the cost of training large models, we only\nhave two comparable training runs at large scale ( Chinchilla andGopher), and we do not have\nadditional tests at intermediate scales. Furthermore, we assume that the e\ufb03cient computational\nfrontier can be described by a power-law relationship between the compute budget, model size, and\nnumberoftrainingtokens. However,weobservesomeconcavityin log\u0000\n\ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61\u0001athighcomputebudgets\n(see Appendix E). This suggests that we may still be overestimating the optimal size of large models.\nFinally, the training runs for our analysis have all been trained on less than an epoch of data; future\nwork may consider the multiple epoch regime. Despite these limitations, the comparison of Chinchilla\ntoGophervalidates our performance predictions, that have thus enabled training a better (and more\n15", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91b665e9-1cac-4ff9-8338-27e2b3d9e9bd": {"__data__": {"id_": "91b665e9-1cac-4ff9-8338-27e2b3d9e9bd", "embedding": null, "metadata": {"page_label": "16", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7cac0a20-b98d-4376-a7a2-eecc378e53c3", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "0dd351a36036b634990f1efefc22f676a9a50cb39519c56ee675b71473ea0921", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58a94e53-e6c2-4ce0-bc16-67e614f0dd78", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "25649ea328ac7dc54ac2ae6904a0bdb2864c69b497b9d03a80e297c0ea16ca4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a5b5df5-5f3f-4826-bd06-ab41668e9cf1", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "ed13be6f4b645c61fb19f0f6f0e172ce040690a936d472710454962f410e8557", "class_name": "RelatedNodeInfo"}}, "hash": "5be8225d5c7e8a84503c85b84ebec2490b356012efd0d82b412d5c99769d31e5", "text": "lightweight) model at the same compute budget.\nThough there has been signi\ufb01cant recent work allowing larger and larger models to be trained,\nour analysis suggests an increased focus on dataset scaling is needed. Speculatively, we expect that\nscaling to larger and larger datasets is only bene\ufb01cial when the data is high-quality. This calls for\nresponsibly collecting larger datasets witha high focus ondataset quality. Larger datasetswill require\nextra care to ensure train-test set overlap is properly accounted for, both in the language modelling\nloss but also with downstream tasks. Finally, training for trillions of tokens introduces many ethical\nand privacy concerns. Large datasets scraped from the web will contain toxic language, biases, and\nprivate information. With even larger datasets being used, the quantity (if not the frequency) of such\ninformation increases, which makes dataset introspection all the more important. Chinchilla does\nsu\ufb00erfrombiasandtoxicitybutinterestinglyitseemslessa\ufb00ectedthan Gopher. Betterunderstanding\nhow performance of large language models and toxicity interact is an important future research\nquestion.\nWhile we have applied our methodology towards the training of auto-regressive language models,\nwe expect that there is a similar trade-o\ufb00 between model size and the amount of data in other\nmodalities. As training large models is very expensive, choosing the optimal model size and training\nsteps beforehand is essential. The methods we propose are easy to reproduce in new settings.\n6. Acknowledgements\nWe\u2019dliketothankJean-baptisteAlayrac,KareemAyoub,ChrisDyer,NandodeFreitas,DemisHassabis,\nGeo\ufb00rey Irving, Koray Kavukcuoglu, Nate Kushman and Angeliki Lazaridou for useful comments on\nthe manuscript. We\u2019d like to thank Andy Brock, Irina Higgins, Michela Paganini, Francis Song, and\nother colleagues at DeepMind for helpful discussions. We are also very grateful to the JAX and XLA\nteam for their support and assistance.\nReferences\nM. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V. Lin, J. Du, S. Iyer, R. Pasunuru,\nG. Anantharaman, X. Li, S. Chen, H. Akin, M. Baines, L. Martin, X. Zhou, P. S. Koura, B. O\u2019Horo,\nJ. Wang, L. Zettlemoyer, M. Diab, Z. Kozareva, and V. Stoyanov. E\ufb03cient Large Scale Language\nModeling with Mixtures of Experts. arXiv:2112.10684, 2021.\nE. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots:\nCan language models be too big? In Proceedings ofthe2021ACMConference onFairness,\nAccountability, andTransparency, pages 610\u2013623, 2021.\nBIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabilities of\nlanguage models. Inpreparation, 2021. URL https://github.com/google/BIG-bench/ .\nY. Bisk, R. Zellers, J. Gao, Y. Choi, et al. PIQA: Reasoning about physical commonsense in natural\nlanguage. In Proceedings oftheAAAIConference onArti\ufb01cial Intelligence , volume 34, pages\n7432\u20137439, 2020.\nS. Borgeaud, A. Mensch, J. Ho\ufb00mann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J.-B.\nLespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang,\nL. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero,\nK. Simonyan, J. W. Rae, E. Elsen, and L. Sifre. Improving language models by retrieving from\ntrillions of tokens. arXiv2112.04426, 2021.\n16", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a5b5df5-5f3f-4826-bd06-ab41668e9cf1": {"__data__": {"id_": "6a5b5df5-5f3f-4826-bd06-ab41668e9cf1", "embedding": null, "metadata": {"page_label": "17", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "337dc3d9-dc1c-4087-b50e-bed6e0fab618", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "bc350194e959bc354d91cf8b6e0e7b01a95a7a65c300cf85d64b0a7c03c97be9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91b665e9-1cac-4ff9-8338-27e2b3d9e9bd", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "5be8225d5c7e8a84503c85b84ebec2490b356012efd0d82b412d5c99769d31e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f14cab1b-e278-4979-814d-3327508f1542", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "82fdbfdf4fd6bbed3c7b388937734609ac6bd94f8e7f6a38ca1df7afd6057ddc", "class_name": "RelatedNodeInfo"}}, "hash": "ed13be6f4b645c61fb19f0f6f0e172ce040690a936d472710454962f410e8557", "text": "J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-\nderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy\nprograms. 2018. URL http://github.com/google/jax .\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot\nlearners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances\ninNeuralInformation Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.,\n2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb49674\n18bfb8ac142f64a-Paper.pdf .\nS. Bubeck. Convex Optimization: Algorithms and Complexity. Foundations andTrendsinMachine\nLearning, 8(3-4):231\u2013357, 2015. URL http://www.nowpublishers.com/article/Detail\ns/MAL-050 .\nA. Clark, D. d. l. Casas, A. Guy, A. Mensch, M. Paganini, J. Ho\ufb00mann, B. Damoc, B. Hechtman,\nT. Cai, S. Borgeaud, G. v. d. Driessche, E. Rutherford, T. Hennigan, M. Johnson, K. Millican,\nA. Cassirer, C. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osindero, O. Vinyals, J. Rae, E. Elsen,\nK. Kavukcuoglu, and K. Simonyan. Uni\ufb01ed scaling laws for routed language models, 2022. URL\nhttps://arxiv.org/abs/2202.01169 .\nC. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring\nthe surprising di\ufb03culty of natural yes/no questions. In Proceedings ofthe2019Conference of\ntheNorthAmerican ChapteroftheAssociation forComputational Linguistics: HumanLanguage\nTechnologies, Volume1(LongandShortPapers), pages 2924\u20132936, 2019.\nN.Du,Y.Huang,A.M.Dai,S.Tong,D.Lepikhin,Y.Xu,M.Krikun,Y.Zhou,A.W.Yu,O.Firat,B.Zoph,\nL. Fedus, M. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. Meier-\nHellstern,T.Duke,L.Dixon,K.Zhang,Q.V.Le,Y.Wu,Z.Chen,andC.Cui. Glam: E\ufb03cientscalingof\nlanguage models with mixture-of-experts, 2021. URL https://arxiv.org/abs/2112.06905 .\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with\nsimple and e\ufb03cient sparsity. arXivpreprintarXiv:2101.03961, 2021.\nL.Gao,S.Biderman, S.Black, L.Golding, T.Hoppe, C.Foster,J.Phang, H.He, A.Thite, N.Nabeshima,\nS. Presser, and C. Leahy. The Pile: An 800GB dataset of diverse text for language modeling.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f14cab1b-e278-4979-814d-3327508f1542": {"__data__": {"id_": "f14cab1b-e278-4979-814d-3327508f1542", "embedding": null, "metadata": {"page_label": "17", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "337dc3d9-dc1c-4087-b50e-bed6e0fab618", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "bc350194e959bc354d91cf8b6e0e7b01a95a7a65c300cf85d64b0a7c03c97be9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a5b5df5-5f3f-4826-bd06-ab41668e9cf1", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "ed13be6f4b645c61fb19f0f6f0e172ce040690a936d472710454962f410e8557", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c23b670-23a4-484c-9698-e366e6f68ed1", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "d0ba0b1cbd99a1bcacec53ceb3b7dd5bf8cd3054980c3777109b03cdf827f8a5", "class_name": "RelatedNodeInfo"}}, "hash": "82fdbfdf4fd6bbed3c7b388937734609ac6bd94f8e7f6a38ca1df7afd6057ddc", "text": "Glam: E\ufb03cientscalingof\nlanguage models with mixture-of-experts, 2021. URL https://arxiv.org/abs/2112.06905 .\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with\nsimple and e\ufb03cient sparsity. arXivpreprintarXiv:2101.03961, 2021.\nL.Gao,S.Biderman, S.Black, L.Golding, T.Hoppe, C.Foster,J.Phang, H.He, A.Thite, N.Nabeshima,\nS. Presser, and C. Leahy. The Pile: An 800GB dataset of diverse text for language modeling. arXiv\npreprintarXiv:2101.00027, 2020.\nS. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating\nneural toxic degeneration in language models. In Findings oftheAssociation forComputational\nLinguistics: EMNLP2020, pages 3356\u20133369, Online, Nov. 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2\n020.findings-emnlp.301 .\nK. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. REALM: Retrieval-augmented language model\npre-training, 2020.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive\nmultitask language understanding. arXivpreprintarXiv:2009.03300, 2020.\nT. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX. 2020. URL http:\n//github.com/deepmind/dm-haiku .\n17", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c23b670-23a4-484c-9698-e366e6f68ed1": {"__data__": {"id_": "4c23b670-23a4-484c-9698-e366e6f68ed1", "embedding": null, "metadata": {"page_label": "18", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ac69fbe-54d7-465c-92d2-fd2fee280ce3", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "5dd23d690a35bedf21e8d785dfd2a8695ff6b3b4a9adb472955061941afe7d1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f14cab1b-e278-4979-814d-3327508f1542", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "82fdbfdf4fd6bbed3c7b388937734609ac6bd94f8e7f6a38ca1df7afd6057ddc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1fb69bf-82ff-4593-84e2-7b6a5545408c", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "45a35e85034d07c19f71b602f30a9c167ab66e11d71c6bd0def621e629fe035c", "class_name": "RelatedNodeInfo"}}, "hash": "d0ba0b1cbd99a1bcacec53ceb3b7dd5bf8cd3054980c3777109b03cdf827f8a5", "text": "D. Hernandez, J. Kaplan, T. Henighan, and S. McCandlish. Scaling laws for transfer, 2021.\nP. J. Huber. Robust Estimation of a Location Parameter. TheAnnalsofMathematical Statistics, 35\n(1):73\u2013101, Mar. 1964. ISSN 0003-4851, 2168-8990. doi: 10.1214/aoms/1177703732. URL\nhttps://projecteuclid.org/journals/annals-of-mathematical-statistics/vol\nume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/11\n77703732.full .\nG. Izacard and E. Grave. Distilling knowledge from reader to retriever for question answering, 2020.\nM.Joshi,E.Choi,D.Weld,andL.Zettlemoyer. TriviaQA:ALargeScaleDistantlySupervisedChallenge\nDataset for Reading Comprehension. arXive-prints, art. arXiv:1705.03551, 2017.\nN. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden,\nA. Borchers, R. Boyle, P.-l. Cantin, C. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb,\nT.V.Ghaemmaghami,R.Gottipati,W.Gulland,R.Hagmann,C.R.Ho,D.Hogberg,J.Hu,R.Hundt,\nD.Hurt,J.Ibarz,A.Ja\ufb00ey,A.Jaworski,A.Kaplan,H.Khaitan,D.Killebrew,A.Koch,N.Kumar,S.Lacy,\nJ.Laudon, J.Law, D.Le, C.Leary, Z.Liu, K.Lucke, A.Lundin, G.MacKean, A.Maggiore, M.Mahony,\nK. Miller, R. Nagarajan, R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick, N. Penukonda,\nA. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani, C. Severn, G. Sizikov, M. Snelham, J. Souter,\nD. Steinberg, A. Swing, M. Tan, G. Thorson, B. Tian, H. Toma, E. Tuttle, V. Vasudevan, R. Walter,\nW.Wang,E.Wilcox,andD.H.Yoon. In-datacenterperformanceanalysisofatensorprocessingunit.\nInProceedings ofthe44thAnnualInternational Symposium onComputer Architecture , ISCA \u201917,\npage1\u201312,NewYork,NY,USA,2017.AssociationforComputingMachinery. ISBN9781450348928.\ndoi: 10.1145/3079856.3080246. URL https://doi.org/10.1145/3079856.3080246 .\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,\nand D. Amodei. Scaling laws for neural language models. arXivpreprintarXiv:2001.08361 , 2020.\nD.P.KingmaandJ.Ba. Adam: Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980 ,\n2014.\nT. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. arXivpreprintarXiv:1808.06226, 2018.\nT. Kwiatkowski, J. Palomaki, O. Red\ufb01eld, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin,\nM. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and\nS. Petrov. Natural questions: a benchmark for question answering research.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1fb69bf-82ff-4593-84e2-7b6a5545408c": {"__data__": {"id_": "c1fb69bf-82ff-4593-84e2-7b6a5545408c", "embedding": null, "metadata": {"page_label": "18", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ac69fbe-54d7-465c-92d2-fd2fee280ce3", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "5dd23d690a35bedf21e8d785dfd2a8695ff6b3b4a9adb472955061941afe7d1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c23b670-23a4-484c-9698-e366e6f68ed1", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "d0ba0b1cbd99a1bcacec53ceb3b7dd5bf8cd3054980c3777109b03cdf827f8a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2977affb-138a-44d2-8c73-23ba3fdd1a25", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "3df2d372599ed22664f2e4203330b5d48d07ed32d8b971507a1cedfdb54e47d4", "class_name": "RelatedNodeInfo"}}, "hash": "45a35e85034d07c19f71b602f30a9c167ab66e11d71c6bd0def621e629fe035c", "text": "Adam: Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980 ,\n2014.\nT. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. arXivpreprintarXiv:1808.06226, 2018.\nT. Kwiatkowski, J. Palomaki, O. Red\ufb01eld, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin,\nM. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and\nS. Petrov. Natural questions: a benchmark for question answering research. Transactions ofthe\nAssociation ofComputational Linguistics, 2019.\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. RACE: Large-scale ReAding comprehension dataset from\nexaminations. In Proceedings ofthe2017Conference onEmpirical Methods inNaturalLanguage\nProcessing , pages 785\u2013794, Copenhagen, Denmark, Sept. 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/D17-1082. URL https://aclanthology.org/D17-1082 .\nY. Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua. The depth-to-width interplay in self-attention.\narXivpreprintarXiv:2006.12467, 2020.\nP. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K\u00fcttler, M. Lewis, W.-t. Yih,\nT. Rockt\u00e4schel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive\nnlp tasks. In Advances inNeuralInformation Processing Systems, volume 33, pages 9459\u20139474,\n2020.\n18", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2977affb-138a-44d2-8c73-23ba3fdd1a25": {"__data__": {"id_": "2977affb-138a-44d2-8c73-23ba3fdd1a25", "embedding": null, "metadata": {"page_label": "19", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1815f9be-960c-48ea-8f5c-790013fc6516", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "5345ca57d480f38f2892c3af1f946aa13e3850912ba632489cd0cfa7d569e330", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1fb69bf-82ff-4593-84e2-7b6a5545408c", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "45a35e85034d07c19f71b602f30a9c167ab66e11d71c6bd0def621e629fe035c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cd3667d-de22-4b44-9159-99a8f73712de", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "ea19dec2f7aba73d4dee7c88966f1e255f92c8557738a8c5006d4956cf81862b", "class_name": "RelatedNodeInfo"}}, "hash": "3df2d372599ed22664f2e4203330b5d48d07ed32d8b971507a1cedfdb54e47d4", "text": "O. Lieber, O. Sharir, B. Lenz, and Y. Shoham. Jurassic-1: Technical details and evaluation. White\nPaper.AI21Labs, 2021.\nS. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods. arXiv\npreprintarXiv:2109.07958, 2021.\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on\nLearning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7 .\nS. McCandlish, J. Kaplan, D. Amodei, and O. D. Team. An empirical model of large-batch training,\n2018.\nS. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. International\nConference onLearning Representations, 2017.\nM.Mitchell,S.Wu,A.Zaldivar,P.Barnes,L.Vasserman,B.Hutchinson,E.Spitzer,I.D.Raji,andT.Ge-\nbru. Modelcardsformodelreporting. In Proceedings oftheconference onfairness,accountability,\nandtransparency, pages 220\u2013229, 2019.\nJ. Nocedal. Updating Quasi-Newton Matrices with Limited Storage. Mathematics ofComputation ,\n35(151):773\u2013782, 1980. ISSN 0025-5718. doi: 10.2307/2006193. URL https://www.jstor.\norg/stable/2006193 .\nD. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda,\nand R. Fern\u00e1ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context,\n2016.\nJ. Rae, S. Borgeaud, T. Cai, K. Millican, J. Ho\ufb00mann, F. Song, J. Aslanides, S. Henderson, R. Ring,\nS. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A.\nHendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor,\nI. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden,\nE. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh,\nE. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev,\nD. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d\u2019Autume, Y. Li,\nT. Terzi, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, J. Bradbury, M. Johnson, L. Weidinger,\nI. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway,\nL.Bennett,D.Hassabis,K.Kavukcuoglu,andG.Irving. Scalinglanguagemodels: Methods,analysis\n& insights from training Gopher. arXiv2112.11446, 2021.\nJ. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, K. Choromanski, V. Likhosherstov, D. Dohan,\nX. Song, A. Gane, T. Sarlos, et al. Compressive transformers for long-range sequence modelling.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cd3667d-de22-4b44-9159-99a8f73712de": {"__data__": {"id_": "6cd3667d-de22-4b44-9159-99a8f73712de", "embedding": null, "metadata": {"page_label": "19", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1815f9be-960c-48ea-8f5c-790013fc6516", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "5345ca57d480f38f2892c3af1f946aa13e3850912ba632489cd0cfa7d569e330", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2977affb-138a-44d2-8c73-23ba3fdd1a25", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "3df2d372599ed22664f2e4203330b5d48d07ed32d8b971507a1cedfdb54e47d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afa700a0-cc75-4678-b80d-d3a76c40a78a", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "39eb615c313f08e90381153a0faffc734b696ff66b2ea952bc8ca6b018d47369", "class_name": "RelatedNodeInfo"}}, "hash": "ea19dec2f7aba73d4dee7c88966f1e255f92c8557738a8c5006d4956cf81862b", "text": "Scalinglanguagemodels: Methods,analysis\n& insights from training Gopher. arXiv2112.11446, 2021.\nJ. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, K. Choromanski, V. Likhosherstov, D. Dohan,\nX. Song, A. Gane, T. Sarlos, et al. Compressive transformers for long-range sequence modelling.\nAdvances inNeuralInformation Processing Systems, 33:6154\u20136158, 2020.\nC.Ra\ufb00el,N.Shazeer,A.Roberts,K.Lee,S.Narang,M.Matena,Y.Zhou,W.Li,andP.J.Liu. Exploring\nthe limits of transfer learning with a uni\ufb01ed text-to-text transformer. JournalofMachine Learning\nResearch, 21(140):1\u201367, 2020a. URL http://jmlr.org/papers/v21/20-074.html .\nC.Ra\ufb00el,N.Shazeer,A.Roberts,K.Lee,S.Narang,M.Matena,Y.Zhou,W.Li,andP.J.Liu. Exploring\nthe limits of transfer learning with a uni\ufb01ed text-to-text transformer. JournalofMachine Learning\nResearch, 21(140):1\u201367, 2020b.\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training\ntrillion parameter models. In SC20:International Conference forHighPerformance Computing,\nNetworking, StorageandAnalysis, pages 1\u201316. IEEE, 2020.\n19", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afa700a0-cc75-4678-b80d-d3a76c40a78a": {"__data__": {"id_": "afa700a0-cc75-4678-b80d-d3a76c40a78a", "embedding": null, "metadata": {"page_label": "20", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9f11827-92a1-4bf9-8481-9bb2e8276320", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "4b390effbf8b3b3d9f67e44d4733b7face6f443f7a6b6e57b249efe95f05fa55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cd3667d-de22-4b44-9159-99a8f73712de", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "ea19dec2f7aba73d4dee7c88966f1e255f92c8557738a8c5006d4956cf81862b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb2b22ac-d07c-4a1e-aabd-d412050a6f46", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "59edd0a10674af93a12fe3cd7d7bc791ad364c2604cb104b6176c1dcb091d2a7", "class_name": "RelatedNodeInfo"}}, "hash": "39eb615c313f08e90381153a0faffc734b696ff66b2ea952bc8ca6b018d47369", "text": "H.RobbinsandS.Monro. AStochasticApproximationMethod. TheAnnalsofMathematical Statistics ,\n22(3):400\u2013407, Sept. 1951.\nR. Rudinger, J. Naradowsky, B. Leonard, and B. Van Durme. Gender bias in coreference resolu-\ntion. InProceedings ofthe2018Conference oftheNorthAmerican ChapteroftheAssociation for\nComputational Linguistics: HumanLanguage Technologies , New Orleans, Louisiana, June 2018.\nAssociation for Computational Linguistics.\nK. Sakaguchi, R. Le Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema\nchallenge at scale. In Proceedings oftheAAAIConference onArti\ufb01cial Intelligence , volume 34,\npages 8732\u20138740, 2020.\nM. Sap, H. Rashkin, D. Chen, R. LeBras, and Y. Choi. SocialIQA: Commonsense reasoning about\nsocialinteractions. Proceedings ofthe2019Conference onEmpirical Methods inNaturalLanguage\nProcessing, 2019.\nC. J. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, and G. E. Dahl. Measuring the e\ufb00ects\nof data parallelism on neural network training. arXivpreprintarXiv:1811.03600, 2018.\nJ. W. Siegel and J. Xu. Approximation rates for neural networks with general activation functions.\nNeuralNetworks , 128:313\u2013321, Aug. 2020. URL https://www.sciencedirect.com/scienc\ne/article/pii/S0893608020301891 .\nS. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye,\nG. Zerveas, V. Korthikanti, E. Zhang, R. Child, R. Y. Aminabadi, J. Bernauer, X. Song, M. Shoeybi,\nY.He, M.Houston, S.Tiwary, andB.Catanzaro. UsingDeepspeedandMegatrontoTrainMegatron-\nturing NLG 530b, A Large-Scale Generative Language Model. arXivpreprintarXiv:2201.11990 ,\n2022.\nJ. Steinhardt. Updates and lessons from AI forecasting, 2021. URL https://bounded-regret.g\nhost.io/ai-forecasting/ .\nY. Tay, M. Dehghani, J. Rao, W. Fedus, S. Abnar, H. W. Chung, S. Narang, D. Yogatama, A. Vaswani,\nand D. Metzler. Scale e\ufb03ciently: Insights from pre-training and \ufb01ne-tuning transformers, 2021.\nR. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker,\nY. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin,\nJ. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, Y. Zhou, C.-C. Chang, I. Krivokon, W. Rusch,\nM. Pickett, K. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zeven-\nbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Ho\ufb00man-John, J. Lee,\nL. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein,\nR. Kurzweil, B. Aguera-Arcas, C. Cui, M. Croak, E. Chi, and Q. Le. LaMDA: Language models for\ndialog applications, 2022.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb2b22ac-d07c-4a1e-aabd-d412050a6f46": {"__data__": {"id_": "bb2b22ac-d07c-4a1e-aabd-d412050a6f46", "embedding": null, "metadata": {"page_label": "20", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9f11827-92a1-4bf9-8481-9bb2e8276320", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "4b390effbf8b3b3d9f67e44d4733b7face6f443f7a6b6e57b249efe95f05fa55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afa700a0-cc75-4678-b80d-d3a76c40a78a", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "39eb615c313f08e90381153a0faffc734b696ff66b2ea952bc8ca6b018d47369", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d2c7fee-73d6-4fd9-9d31-e35c1116bf74", "node_type": "1", "metadata": {"page_label": "21", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "ebb1651abaf7381d7a867626927460dd25c551fdcbe080dcfac354cccabc0158", "class_name": "RelatedNodeInfo"}}, "hash": "59edd0a10674af93a12fe3cd7d7bc791ad364c2604cb104b6176c1dcb091d2a7", "text": "Chang, I. Krivokon, W. Rusch,\nM. Pickett, K. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zeven-\nbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Ho\ufb00man-John, J. Lee,\nL. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein,\nR. Kurzweil, B. Aguera-Arcas, C. Cui, M. Croak, E. Chi, and Q. Le. LaMDA: Language models for\ndialog applications, 2022.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin.\nAttentionisallyouneed. In Advances inneuralinformation processing systems, pages5998\u20136008,\n2017.\nL. Weidinger, J. Mellor, M. Rauh, C. Gri\ufb03n, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese, B. Balle,\nA.Kasirzadeh,Z.Kenton,S.Brown,W.Hawkins,T.Stepleton,C.Biles,A.Birhane,J.Haas,L.Rimell,\nL. A. Hendricks, W. Isaac, S. Legassick, G. Irving, and I. Gabriel. Ethical and social risks of harm\nfrom language models. arXivsubmission, 2021.\n20", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d2c7fee-73d6-4fd9-9d31-e35c1116bf74": {"__data__": {"id_": "8d2c7fee-73d6-4fd9-9d31-e35c1116bf74", "embedding": null, "metadata": {"page_label": "21", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d8af2b0-6414-40a6-b5a5-0d5bc422e002", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "84639504fccedb1b03a4b0717c1afc70f035c4a1ea6a22c414ef5c233ec98edc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb2b22ac-d07c-4a1e-aabd-d412050a6f46", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "59edd0a10674af93a12fe3cd7d7bc791ad364c2604cb104b6176c1dcb091d2a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97c5a966-cf49-4ac8-be31-fb4a105f9472", "node_type": "1", "metadata": {"page_label": "22", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "69c6c2416cb24aaa8d6c34fd4db3f75ba63812341927a1baf504bb94d4f343a6", "class_name": "RelatedNodeInfo"}}, "hash": "ebb1651abaf7381d7a867626927460dd25c551fdcbe080dcfac354cccabc0158", "text": "J. Welbl, A. Glaese, J. Uesato, S. Dathathri, J. Mellor, L. A. Hendricks, K. Anderson, P. Kohli, B. Coppin,\nand P.-S. Huang. Challenges in detoxifying language models. In Findings oftheAssociation for\nComputational Linguistics: EMNLP2021, pages 2447\u20132469, Punta Cana, Dominican Republic,\nNov. 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.\nfindings-emnlp.210 .\nA. Xu, E. Pathak, E. Wallace, S. Gururangan, M. Sap, and D. Klein. Detoxifying language models\nrisks marginalizing minority voices. In Proceedings ofthe2021Conference oftheNorthAmerican\nChapteroftheAssociation forComputational Linguistics: HumanLanguage Technologies , pages\n2390\u20132397, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021\n.naacl-main.190. URL https://aclanthology.org/2021.naacl-main.190 .\nG. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen, and J. Gao.\nTuninglargeneuralnetworksviazero-shothyperparametertransfer. InA.Beygelzimer, Y.Dauphin,\nP. Liang, and J. W. Vaughan, editors, Advances inNeuralInformation Processing Systems, 2021.\nURLhttps://openreview.net/forum?id=Bx6qKuBM2AD .\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really \ufb01nish\nyour sentence? In Proceedings ofthe57thAnnualMeetingoftheAssociation forComputational\nLinguistics, 2019.\nG. Zhang, L. Li, Z. Nado, J. Martens, S. Sachdeva, G. Dahl, C. Shallue, and R. B. Grosse. Which\nalgorithmic choices matter at which batch sizes? insights from a noisy quadratic model. In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances\ninNeuralInformation Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https:\n//proceedings.neurips.cc/paper/2019/file/e0eacd983971634327ae1819ea8b621\n4-Paper.pdf .\nB. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus. Designing e\ufb00ective\nsparse expert models, 2022.\n21", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97c5a966-cf49-4ac8-be31-fb4a105f9472": {"__data__": {"id_": "97c5a966-cf49-4ac8-be31-fb4a105f9472", "embedding": null, "metadata": {"page_label": "22", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73cd2ddd-601f-43cc-b123-6f3d58f21df9", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "5a53cbf5d1d33fe9005eff37e8983720d44255c263e987b2e799e7a3ca2ffb64", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d2c7fee-73d6-4fd9-9d31-e35c1116bf74", "node_type": "1", "metadata": {"page_label": "21", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "ebb1651abaf7381d7a867626927460dd25c551fdcbe080dcfac354cccabc0158", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d37074ae-af0d-4880-872e-db300881db35", "node_type": "1", "metadata": {"page_label": "23", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "775cc090466eb4b0b55408422586c40bd4d9cc93d0d80fa5fec92dd3a01a26eb", "class_name": "RelatedNodeInfo"}}, "hash": "69c6c2416cb24aaa8d6c34fd4db3f75ba63812341927a1baf504bb94d4f343a6", "text": "Appendix\nA. Training dataset\nIn Table A1 we show the training dataset makeup used for Chinchilla and all scaling runs. Note that\nboth the MassiveWeb and Wikipedia subsets are both used for more than one epoch.\nDisk Size Documents Sampling proportion Epochs in 1.4T tokens\nMassiveWeb 1.9 TB 604M 45% (48%) 1.24\nBooks 2.1 TB 4M 30% (27%) 0.75\nC4 0.75 TB 361M 10% (10%) 0.77\nNews 2.7 TB 1.1B 10% (10%) 0.21\nGitHub 3.1 TB 142M 4% (3%) 0.13\nWikipedia 0.001 TB 6M 1% (2%) 3.40\nTable A1jMassiveText data makeup. For each subset of MassiveText , we list its total disk size, the\nnumber of documents and the sampling proportion used during training\u2014we use a slightly di\ufb00erent\ndistribution than in Rae et al. (2021) (shown in parenthesis). In the rightmost column show the\nnumber of epochs that are used in 1.4 trillion tokens.\nB. Optimal cosine cycle length\nOne key assumption is made on the cosine cycle length and the corresponding learning rate drop\n(we use a 10\u0002learning rate decay in line with Rae et al. (2021)).9We \ufb01nd that setting the cosine\ncycle length too much longer than the target number of training steps results in sub-optimally trained\nmodels, as shown in Figure A1. As a result, we assume that an optimally trained model will have the\ncosine cycle length correctly calibrated to the maximum number of steps, given the FLOP budget; we\nfollow this rule in our main analysis.\nC. Consistency of scaling results across datasets\nWeshowscalingresultsfromanIsoFLOP(Approach2)analysisaftertrainingontwodi\ufb00erentdatasets:\nC4 (Ra\ufb00el et al., 2020b) and GitHub code (we show results with data from Rae et al. (2021)), results\nare shown in Table A2. For both set of experiments using subsets of MassiveText , we use the same\ntokenizer as the MassiveText experiments.\nWe\ufb01ndthatthescalingbehaviouronthesedatasetsisverysimilartowhatwefoundon MassiveText ,\nas shown in Figure A2 and Table A2. This suggests that our results are independent of the dataset as\nlong as one does not train for more than one epoch.\n9We \ufb01nd the di\ufb00erence between decaying by 10\u0002and decaying to 0.0 (over the same number of steps) to be small,\nthough decaying by a factor of 10\u0002to be slightly more performant. Decaying by less ( 5\u0002) is clearly worse.\n22", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d37074ae-af0d-4880-872e-db300881db35": {"__data__": {"id_": "d37074ae-af0d-4880-872e-db300881db35", "embedding": null, "metadata": {"page_label": "23", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae03e0cd-6484-4f01-8ba6-1e8ceebe5b09", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "67952238f18c532ed40d7dee986b5988ee5c2cdfc4fe124d063559af76c999c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97c5a966-cf49-4ac8-be31-fb4a105f9472", "node_type": "1", "metadata": {"page_label": "22", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "69c6c2416cb24aaa8d6c34fd4db3f75ba63812341927a1baf504bb94d4f343a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c01521d1-ae3f-4321-9d9f-1030f1080c2b", "node_type": "1", "metadata": {"page_label": "24", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "3c420cca9801e187c6ccce140cce3150c5513a36c40d65060734a320b8f9f237", "class_name": "RelatedNodeInfo"}}, "hash": "775cc090466eb4b0b55408422586c40bd4d9cc93d0d80fa5fec92dd3a01a26eb", "text": "0 2 4 6 8\nMillion Sequences0.00.20.40.60.81.0Learning Rate/Max LR\n0 2 4 6 8\nMillion Sequences2.702.752.802.852.902.953.00Training Loss\n0 2 4 6\nMillion Sequences2.802.852.902.953.003.053.103.153.20C4 LossCosine Cycle Length\n1.0\u00d7 num. steps\n1.1\u00d7 num. steps\n1.25\u00d7 num. steps\n1.5\u00d7 num. steps\n2.0\u00d7 num. steps\n5.0\u00d7 num. steps\n0.0 2.5 5.0 7.5 10.0 12.5\nMillion Sequences0.00.20.40.60.81.0Learning Rate/Max LR\n0.0 2.5 5.0 7.5 10.0 12.5\nMillion Sequences2.702.752.802.852.902.953.00Training Loss\n0.0 2.5 5.0 7.5 10.0 12.5\nMillion Sequences2.802.852.902.953.003.053.103.153.20C4 LossFigure A1jGrid over cosine cycle length. We show 6 curves with the cosine cycle length set to 1,\n1.1,1.25,1.5,2,and5 \u0002longerthanthetargetnumberoftrainingsteps. Whenthecosinecyclelength\nis too long, and the learning rate does not drop appropriately, then performance is impaired. We \ufb01nd\nthat overestimating the number of training steps beyond 25% leads to clear drops in performance.\nWe show results where we have set the number of training steps to two di\ufb00erent values (top and\nbottom).\n100M 300M 1B 3B 6B 30B\nParameters2.02.22.42.62.83.03.2C4 Training Loss\n1e19\n1e20\n6e20\n1e21\n10171019102110231025\nFLOPs100M1B10B100B1TParameters\n73B\n10171019102110231025\nFLOPs100M1B10B100B1T10TTokens\n1.3T\n100M 300M 1B 3B 6B 30B\nParameters0.20.30.40.50.60.70.80.91.0GitHub Training Loss\n1e19\n1e20\n6e20\n1e21\n10171019102110231025\nFLOPs100M1B10B100B1TParameters\n59B\n10171019102110231025\nFLOPs100M1B10B100B1T10TTokens\n1.6T\nFigureA2jC4andGitHubIsoFLOPcurves. UsingtheC4dataset(Ra\ufb00eletal.,2020b)andaGitHub\ndataset (Rae et al., 2021), we generate 4 IsoFLOP pro\ufb01les and show the parameter and token count\nscaling, as in Figure 3. Scaling coe\ufb03cients are shown in Table A2.\n23", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c01521d1-ae3f-4321-9d9f-1030f1080c2b": {"__data__": {"id_": "c01521d1-ae3f-4321-9d9f-1030f1080c2b", "embedding": null, "metadata": {"page_label": "24", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f79e085a-e4ba-4c3e-8364-6d874eaa357a", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "09d1eb46f70456affd15a359cef68a40e5455f6eb0f1bc59cba9179ce6ec2a1c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d37074ae-af0d-4880-872e-db300881db35", "node_type": "1", "metadata": {"page_label": "23", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "775cc090466eb4b0b55408422586c40bd4d9cc93d0d80fa5fec92dd3a01a26eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84fa3ab5-e358-4ddb-ac8b-6953e7a3db47", "node_type": "1", "metadata": {"page_label": "25", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "2afb12c5401b46fa7cbf3ecab225c14f724cf5e3efb720cb4ec9aae8ee22cb7f", "class_name": "RelatedNodeInfo"}}, "hash": "3c420cca9801e187c6ccce140cce3150c5513a36c40d65060734a320b8f9f237", "text": "Approach Coef. \ud835\udc4ewhere\ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61/\ud835\udc36\ud835\udc4eCoef.\ud835\udc4fwhere\ud835\udc37\ud835\udc5c\ud835\udc5d\ud835\udc61/\ud835\udc36\ud835\udc4f\nC4 0.50 0.50\nGitHub 0.53 0.47\nKaplan et al. (2020) 0.73 0.27\nTable A2jEstimated parameter and data scaling with increased training compute on two al-\nternate datasets. The listed values are the exponents, \ud835\udc4eand\ud835\udc4f, on the relationship \ud835\udc41\ud835\udc5c\ud835\udc5d\ud835\udc61/\ud835\udc36\ud835\udc4eand\n\ud835\udc37\ud835\udc5c\ud835\udc5d\ud835\udc61/\ud835\udc36\ud835\udc4f. Using IsoFLOP pro\ufb01les, we estimate the scaling on two di\ufb00erent datasets.\nD. Details on the scaling analyses\nD.1. Approach 1: Fixing model sizes and varying training sequences\nWe use a maximum learning rate of 2\u000210\u00004for the smallest models and 1\u009325\u000210\u00004for the largest\nmodels. Inallcases,thelearningratedropsbyafactorof 10\u0002duringtraining,usingacosineschedule.\nWemaketheassumptionthatthecosinecyclelengthshouldbeapproximatelymatchedtothenumber\nof training steps. We \ufb01nd that when the cosine cycle overshoots the number of training steps by more\nthan 25%, performance is noticeably degraded\u2014see Figure A1.10We use Gaussian smoothing with a\nwindow length of 10 steps to smooth the training curve.\nD.2. Approach 3: Parametric \ufb01tting of the loss\nIn this section, we \ufb01rst show how Equation (2)can be derived. We repeat the equation below for\nclarity,\n\u02c6\ud835\udc3f\u00b9\ud835\udc41\u0094\ud835\udc37\u00ba,\ud835\udc38\u00b8\ud835\udc34\n\ud835\udc41\ud835\udefc\u00b8\ud835\udc35\n\ud835\udc37\ud835\udefd\u0094 (5)\nbased on a decomposition of the expected risk between a function approximation term and an\noptimisation suboptimality term. We then give details on the optimisation procedure for \ufb01tting the\nparameters.\nLoss decomposition. Formally, we consider the task of predicting the next token \ud835\udc662Ybased on\nthe previous tokens in a sequence \ud835\udc652Y\ud835\udc60, with\ud835\udc60varying from 0to\ud835\udc60max\u2014the maximum sequence\nlength. We consider a distribution \ud835\udc432D\u00b9X\u0002Y\u00ba of tokens inYand their past in X. A predictor\n\ud835\udc53:X!D\u00b9Y\u00ba computes the probability of each token given the past sequence. The Bayes classi\ufb01er,\n\ud835\udc53\u2605, minimizes the cross-entropy of \ud835\udc53\u00b9\ud835\udc65\u00bawith the observed tokens \ud835\udc66, with expectation taken on the\nwhole data distribution. We let \ud835\udc3fbe the expected risk\n\ud835\udc3f\u00b9\ud835\udc53\u00ba,\ud835\udd3c\u00bblog\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc66\u00bc\u0094and set \ud835\udc53\u2605,argmin\n\ud835\udc532F\u00b9X\u0094D\u00b9Y\u00ba\u00ba\ud835\udc3f\u00b9\ud835\udc53\u00ba\u0093 (6)\nThe set of all transformers of size \ud835\udc41, that we denoteH\ud835\udc41, forms a subset of all functions that map\nsequences to distributions of tokens X!D\u00b9Y\u00ba . Fitting a transformer of size \ud835\udc41on the expected risk\n\ud835\udc3f\u00b9\ud835\udc53\u00baamounts to minimizing such risk on a restricted functional space\n\ud835\udc53\ud835\udc41,argmin\n\ud835\udc532H\ud835\udc41\ud835\udc3f\u00b9\ud835\udc53\u00ba\u0093 (7)\nWhen we observe a dataset \u00b9\ud835\udc65\ud835\udc56\u0094\ud835\udc66\ud835\udc56\u00ba\ud835\udc56\ud835\udc562\u00bb1\u0094\ud835\udc37\u00bcof size\ud835\udc37, we do not have access to \ud835\udd3c\ud835\udc43, but instead to the\nempirical expectation \u02c6\ud835\udd3c\ud835\udc37over the empirical distribution \u02c6\ud835\udc43\ud835\udc37. What happens when we are given \ud835\udc37\n10This further emphasises the point of not only determining model size, but also training length before training begins.\n24", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84fa3ab5-e358-4ddb-ac8b-6953e7a3db47": {"__data__": {"id_": "84fa3ab5-e358-4ddb-ac8b-6953e7a3db47", "embedding": null, "metadata": {"page_label": "25", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06dde59c-1d62-4992-84f4-2d56e4115733", "node_type": "4", "metadata": {"page_label": "25", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "7b04dc8b55354486eb211cd7072f3c0cc782aff8b39a6b9921bc192662845d9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c01521d1-ae3f-4321-9d9f-1030f1080c2b", "node_type": "1", "metadata": {"page_label": "24", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "3c420cca9801e187c6ccce140cce3150c5513a36c40d65060734a320b8f9f237", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2828e88-65e6-4051-96b1-8aee312e38aa", "node_type": "1", "metadata": {"page_label": "25", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "99883f913f39c37e002956990d8ff5ff1dbed6e02355dc3a110edae451691bea", "class_name": "RelatedNodeInfo"}}, "hash": "2afb12c5401b46fa7cbf3ecab225c14f724cf5e3efb720cb4ec9aae8ee22cb7f", "text": "datapoints that we can only see once, and when we constrain the size of the hypothesis space to be\n\ud835\udc41-dimensional? Wearemakingstepstowardminimizingtheempiricalriskwithina\ufb01nite-dimensional\nfunctional spaceH\ud835\udc41:\n\u02c6\ud835\udc3f\ud835\udc37\u00b9\ud835\udc53\u00ba,\u02c6\ud835\udd3c\ud835\udc37\u00bblog\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc66\u00bc\u0094setting \u02c6\ud835\udc53\ud835\udc41\u0094\ud835\udc37,argmin\n\ud835\udc532H\ud835\udc41\u02c6\ud835\udc3f\ud835\udc37\u00b9\ud835\udc53\u00ba\u0093 (8)\nWe are never able to obtain \u02c6\ud835\udc53\ud835\udc41\u0094\ud835\udc37as we typically perform a single epoch over the dataset of size \ud835\udc37.\nInstead, be obtain \u00af\ud835\udc53\ud835\udc41\u0094\ud835\udc37, which is the result of applying a certain number of gradient steps based on\nthe\ud835\udc37datapoints\u2014the number of steps to perform depends on the gradient batch size, for which we\nuse well-tested heuristics.\nUsing the Bayes-classi\ufb01er \ud835\udc53\u2605, the expected-risk minimizer \ud835\udc53\ud835\udc41and the \u201csingle-epoch empirical-risk\nminimizer\u201d \u00af\ud835\udc53\ud835\udc41\u0094\ud835\udc37, we can \ufb01nally decompose the loss \ud835\udc3f\u00b9\ud835\udc41\u0094\ud835\udc37\u00bainto\n\ud835\udc3f\u00b9\ud835\udc41\u0094\ud835\udc37\u00ba,\ud835\udc3f\u00b9\u00af\ud835\udc53\ud835\udc41\u0094\ud835\udc37\u00ba=\ud835\udc3f\u00b9\ud835\udc53\u2605\u00ba\u00b8\u0000\n\ud835\udc3f\u00b9\ud835\udc53\ud835\udc41\u00ba\u0000\ud835\udc3f\u00b9\ud835\udc53\u2605\u00ba\u0001\u00b8\u0000\n\ud835\udc3f\u00b9\u00af\ud835\udc53\ud835\udc41\u0094\ud835\udc37\u00ba\u0000\ud835\udc3f\u00b9\ud835\udc53\ud835\udc41\u00ba\u0001\n\u0093 (9)\nThe loss comprises three terms: the Bayes risk, i.e. the minimal loss achievable for next-token\nprediction on the full distribution \ud835\udc43, a.k.a the \u201centropy of natural text.\u201d; a functional approximation\nterm that depends on the size of the hypothesis space; \ufb01nally, a stochastic approximation term that\ncapturesthesuboptimalityofminimizing \u02c6\ud835\udc3f\ud835\udc37insteadof \ud835\udc3f,andofmakingasingleepochontheprovided\ndataset.\nExpected forms of the loss terms. In the decomposition (9), the second term depends entirely on\nthe number of parameters \ud835\udc41that de\ufb01nes the size of the functional approximation space. On the set\nof two-layer neural networks , it is expected to be proportional to1\n\ud835\udc411\u009d2(Siegel and Xu, 2020). Finally,\ngiven that it corresponds to early stopping in stochastic \ufb01rst order methods, the third term should\nscale as the convergence rate of these methods, which is lower-bounded by1\n\ud835\udc371\u009d2(Robbins and Monro,\n1951) (and may attain the bound). This convergence rate is expected to be dimension free (see e.g.\nBubeck, 2015, for a review) and depends only on the loss smoothness; hence we assume that the\nsecond term only depends on \ud835\udc37in (2). Empirically, we \ufb01nd after \ufb01tting (2) that\n\ud835\udc3f\u00b9\ud835\udc41\u0094\ud835\udc37\u00ba=\ud835\udc38\u00b8\ud835\udc34\n\ud835\udc410\u009334\u00b8\ud835\udc35\n\ud835\udc370\u009328\u0094 (10)\nwith\ud835\udc38=1\u009369,\ud835\udc34=406\u00934,\ud835\udc35=410\u00937. We note that the parameter/data coe\ufb03cients are both lower\nthan1\n2; this is expected for the data-e\ufb03ciency coe\ufb03cient (but far from the known lower-bound).\nFuture models and training approaches should endeavor to increase these coe\ufb03cients.\nFitting the decomposition to data.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2828e88-65e6-4051-96b1-8aee312e38aa": {"__data__": {"id_": "a2828e88-65e6-4051-96b1-8aee312e38aa", "embedding": null, "metadata": {"page_label": "25", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06dde59c-1d62-4992-84f4-2d56e4115733", "node_type": "4", "metadata": {"page_label": "25", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "7b04dc8b55354486eb211cd7072f3c0cc782aff8b39a6b9921bc192662845d9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84fa3ab5-e358-4ddb-ac8b-6953e7a3db47", "node_type": "1", "metadata": {"page_label": "25", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "2afb12c5401b46fa7cbf3ecab225c14f724cf5e3efb720cb4ec9aae8ee22cb7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bf48011-656f-4851-aa36-f5eab3d37d77", "node_type": "1", "metadata": {"page_label": "26", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "02267b37fd1316ab49d9e36919d52e2c9b3a0f94bc70f7860b090ad1aab28bf0", "class_name": "RelatedNodeInfo"}}, "hash": "99883f913f39c37e002956990d8ff5ff1dbed6e02355dc3a110edae451691bea", "text": "Empirically, we \ufb01nd after \ufb01tting (2) that\n\ud835\udc3f\u00b9\ud835\udc41\u0094\ud835\udc37\u00ba=\ud835\udc38\u00b8\ud835\udc34\n\ud835\udc410\u009334\u00b8\ud835\udc35\n\ud835\udc370\u009328\u0094 (10)\nwith\ud835\udc38=1\u009369,\ud835\udc34=406\u00934,\ud835\udc35=410\u00937. We note that the parameter/data coe\ufb03cients are both lower\nthan1\n2; this is expected for the data-e\ufb03ciency coe\ufb03cient (but far from the known lower-bound).\nFuture models and training approaches should endeavor to increase these coe\ufb03cients.\nFitting the decomposition to data. We e\ufb00ectively minimize the following problem\nmin\n\ud835\udc4e\u0094\ud835\udc4f\u0094\ud835\udc52\u0094\ud835\udefc\u0094\ud835\udefd\u2211\ufe01\nRun\ud835\udc56Huber\ud835\udeff\u0010\nLSE\u0000\n\ud835\udc4e\u0000\ud835\udefclog\ud835\udc41\ud835\udc56\u0094\ud835\udc4f\u0000\ud835\udefdlog\ud835\udc37\ud835\udc56\u0094\ud835\udc52\u0001\u0000log\ud835\udc3f\ud835\udc56\u0011\n\u0094 (11)\nwhere\ud835\udc3f\ud835\udc46\ud835\udc38is the log-sum-exp operator. We then set \ud835\udc34\u0094\ud835\udc35\u0094\ud835\udc38 =exp\u00b9\ud835\udc4e\u00ba\u0094exp\u00b9\ud835\udc4f\u00ba\u0094exp\u00b9\ud835\udc52\u00ba.\nWe use the LBFGS algorithm to \ufb01nd local minima of the objective above, started on a grid\nof initialisation given by: \ud835\udefc2 f0\u0093\u00940\u00935\u0094\u0093\u0093\u0093\u0094 2\u0093g,\ud835\udefd2 f0\u0093\u00940\u00935\u0094\u0093\u0093\u0093\u0094 2\u0093g,\ud835\udc522 f\u0000 1\u0093\u0094\u0000\u00935\u0094\u0093\u0093\u0093\u0094 1\u0093g,\ud835\udc4e2\nf0\u00945\u0094\u0093\u0093\u0093\u0094 25g, and\ud835\udc4f2f0\u00945\u0094\u0093\u0093\u0093\u0094 25g. We \ufb01nd that the optimal initialisation is not on the boundary of\nour initialisation sweep.\nWe use\ud835\udeff=10\u00003for the Huber loss. We \ufb01nd that using larger values of \ud835\udeffpushes the model to\nover\ufb01t the small compute regime and poorly predict held-out data from larger runs. We \ufb01nd that\nusing a\ud835\udeffsmaller than 10\u00003does not impact the resulting predictions.\n25", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bf48011-656f-4851-aa36-f5eab3d37d77": {"__data__": {"id_": "3bf48011-656f-4851-aa36-f5eab3d37d77", "embedding": null, "metadata": {"page_label": "26", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75dd8a66-ad33-40a8-a872-06a9238ef1be", "node_type": "4", "metadata": {"page_label": "26", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "8640f3d3c3b74927fe739e37de3eda61a4324018f2532c8cba4ee5ef7f2fd945", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2828e88-65e6-4051-96b1-8aee312e38aa", "node_type": "1", "metadata": {"page_label": "25", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "99883f913f39c37e002956990d8ff5ff1dbed6e02355dc3a110edae451691bea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49c96674-a0d5-49a8-b790-b39101aa00cb", "node_type": "1", "metadata": {"page_label": "27", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "d0a707e47cd1877542c870bce6411801ffea15f7f56b9cfe69034fb3bc4ace3b", "class_name": "RelatedNodeInfo"}}, "hash": "02267b37fd1316ab49d9e36919d52e2c9b3a0f94bc70f7860b090ad1aab28bf0", "text": "D.3. Predicted compute optimal frontier for all three methods\nFor Approaches 2 and 3, we show the estimated model size and number of training tokens for a\nvariety of compute budgets in Table A3. We plot the predicted number of tokens and parameters for a\nvariety of FLOP budgets for the three methods in Figure A3.\nApproach 2 Approach 3\nParameters FLOPs Tokens FLOPs Tokens\n400 Million 1.84e+19 7.7 Billion 2.21e+19 9.2 Billion\n1 Billion 1.20e+20 20.0 Billion 1.62e+20 27.1 Billion\n10 Billion 1.32e+22 219.5 Billion 2.46e+22 410.1 Billion\n67 Billion 6.88e+23 1.7 Trillion 1.71e+24 4.1 Trillion\n175 Billion 4.54e+24 4.3 Trillion 1.26e+24 12.0 Trillion\n280 Billion 1.18e+25 7.1 Trillion 3.52e+25 20.1 Trillion\n520 Billion 4.19e+25 13.4 Trillion 1.36e+26 43.5 Trillion\n1 Trillion 1.59e+26 26.5 Trillion 5.65e+26 94.1 Trillion\n10 Trillion 1.75e+28 292.0 Trillion 8.55e+28 1425.5 Trillion\nTableA3jEstimatedoptimaltrainingFLOPsandtrainingtokensforvariousmodelsizes. Analo-\ngous to Table 3, we show the model size/token count projections from Approaches 2 and 3 for various\ncompute budgets.\n.\n1010101110121013\nTokens108109101010111012Parameters\n1e+181e+191e+201e+211e+221e+231e+241e+251e+26 Approach 1\nApproach 2\nApproach 3\nChinchilla\nGopher\nGPT-3\nMegatron-Turing NLG\nFigure A3jOptimal number of tokens and parameters for a training FLOP budget. For a \ufb01xed\nFLOP budget, we show the optimal number of tokens and parameters as predicted by Approaches 1,\n2, and 3. For an alternate representation, see Figure 1.\nD.4. Small-scale comparison to Kaplan et al.(2020)\nFor1021FLOPs, we perform a head-to-head comparison of a model predicted by Approach 1 and\nthat predicted by Kaplan et al. (2020). For both models, we use a batch size of 0.5M tokens and a\n26", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49c96674-a0d5-49a8-b790-b39101aa00cb": {"__data__": {"id_": "49c96674-a0d5-49a8-b790-b39101aa00cb", "embedding": null, "metadata": {"page_label": "27", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce1291ec-6f1b-4c66-aa69-8728baa54296", "node_type": "4", "metadata": {"page_label": "27", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "1c60afd445fba272228b0f8b3c4ca31206a4666507ab01e6b49b050676dee9c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bf48011-656f-4851-aa36-f5eab3d37d77", "node_type": "1", "metadata": {"page_label": "26", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "02267b37fd1316ab49d9e36919d52e2c9b3a0f94bc70f7860b090ad1aab28bf0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a72e312-5571-4322-b5a9-979f57fa83de", "node_type": "1", "metadata": {"page_label": "28", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "560a8cbe099af2a5c10cdb1878751a270194d28c7d22bf1219c5cbaf93c398bb", "class_name": "RelatedNodeInfo"}}, "hash": "d0a707e47cd1877542c870bce6411801ffea15f7f56b9cfe69034fb3bc4ace3b", "text": "maximum learning rate of 1\u00935\u000210\u00004that decays by 10\u0002. From Kaplan et al. (2020), we \ufb01nd that\nthe optimal model size should be 4.68 billion parameters. From our approach 1, we estimate a 2.86\nbillion parameter model should be optimal. We train a 4.74 billion parameter and a 2.80 billion\nparameter transformer to test this hypothesis, using the same depth-to-width ratio to avoid as many\nconfounding factors as possible. We \ufb01nd that our predicted model outperforms the model predicted\nby Kaplan et al. (2020) as shown in Figure A4.\n0 1 2\nSequences 1e72.22.32.42.52.62.72.8Training Loss\n0.0 0.2 0.4 0.6 0.8 1.0\nFLOPs \u00d710212.22.32.42.52.62.72.8Training LossKaplan et al (2020)\nApproach 1\nFigure A4jComparison to Kaplan et al. (2020) at 1021FLOPs.We train 2.80 and 4.74 billion\nparameter transformers predicted as optimal for 1021FLOPs by Approach 1 and by Kaplan et al.\n(2020). We \ufb01nd that our prediction results in a more performant model at the end of training.\nE. Curvature of the FLOP-loss frontier\nWe observe that as models increase there is a curvature in the FLOP-minimal loss frontier. This means\nthat projections from very small models lead to di\ufb00erent predictions than those from larger models.\nIn Figure A5 we show linear \ufb01ts using the \ufb01rst, middle, and \ufb01nal third of frontier-points. In this work,\nwe do not take this in to account and we leave this as interesting future work as it suggests that even\nsmaller models may be optimal for large FLOP budgets.\nF. FLOPs computation\nWe include all training FLOPs, including those contributed to by the embedding matrices, in our\nanalysis. Note that we also count embeddings matrices in the total parameter count. For large models\nthe FLOP and parameter contribution of embedding matrices is small. We use a factor of 2 to describe\nthe multiply accumulate cost. For the forward pass, we consider contributions from:\n\u2022Embeddings\n\u20132\u0002seq_len\u0002vocab_size\u0002d_model\n\u2022Attention (Single Layer)\n\u2013 Key, query and value projections :2\u00023\u0002seq_len\u0002d_model\u0002\u00b9key_size\u0002num_heads\u00ba\n27", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a72e312-5571-4322-b5a9-979f57fa83de": {"__data__": {"id_": "6a72e312-5571-4322-b5a9-979f57fa83de", "embedding": null, "metadata": {"page_label": "28", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "16287717-1851-44fb-9b74-fd4e34938784", "node_type": "4", "metadata": {"page_label": "28", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "402a0856ef3736b0be6ac67da050f36bd1ec6eecde346dfd415e0c771bfe6a81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49c96674-a0d5-49a8-b790-b39101aa00cb", "node_type": "1", "metadata": {"page_label": "27", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "d0a707e47cd1877542c870bce6411801ffea15f7f56b9cfe69034fb3bc4ace3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d2dfcc9-a505-4d2e-9503-80ee8d75615d", "node_type": "1", "metadata": {"page_label": "29", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "8fb83cc173b6e082a597c7a43a28073eb56aa816e1fcf7ee1a79e3da9af09e38", "class_name": "RelatedNodeInfo"}}, "hash": "560a8cbe099af2a5c10cdb1878751a270194d28c7d22bf1219c5cbaf93c398bb", "text": "101710181019102010211022\nFLOPS2.02.53.03.54.04.55.05.56.0Training loss\n7525050010002500500010000\nMillion Parameters\nFigure A5jTraining curve envelopes. We \ufb01t to the \ufb01rst third (orange), the middle third (green),\nand the last third (blue) of all points along the loss frontier. We plot only a subset of the points.\n\u2013 Key @ Query logits :2\u0002seq_len\u0002seq_len\u0002\u00b9key_size\u0002num_heads\u00ba\n\u2013 Softmax :3\u0002num_heads\u0002seq_len\u0002seq_len\n\u2013 Softmax @ query reductions :2\u0002seq_len\u0002seq_len\u0002\u00b9key_size\u0002num_heads\u00ba\n\u2013 Final Linear :2\u0002seq_len\u0002\u00b9key_size\u0002num_heads\u00ba\u0002d_model\n\u2022Dense Block (Single Layer)\n\u20132\u0002seq_len\u0002\u00b9d_model\u0002\ufb00w_size\u00b8d_model\u0002\ufb00w_size\u00ba\n\u2022Final Logits\n\u20132\u0002seq_len\u0002d_model\u0002vocab_size\n\u2022TotalforwardpassFLOPs: embeddings\u00b8num_layers\u0002\u00b9total_attention\u00b8dense_block\u00ba+logits\nAs in Kaplan et al. (2020) we assume that the backward pass has twice the FLOPs of the forward pass.\nWe show a comparison between our calculation and that using the common approximation \ud835\udc36=6\ud835\udc37\ud835\udc41\n(Kaplan et al., 2020) where \ud835\udc36is FLOPs,\ud835\udc37is the number of training tokens, and \ud835\udc41is the number of\nparameters in Table A4. We \ufb01nd the di\ufb00erences in FLOP calculation to be very small and they do not\nimpact our analysis. Compared to the results presented in Rae et al. (2021), we use a slightly more\nParameters num_layers d_model \ufb00w_size num_heads k/q size FLOP Ratio (Ours/ 6\ud835\udc41\ud835\udc37)\n73M 10 640 2560 10 64 1.03\n305M 20 1024 4096 16 64 1.10\n552M 24 1280 5120 10 128 1.08\n1.1B 26 1792 7168 14 128 1.04\n1.6B 28 2048 8192 16 128 1.03\n6.8B 40 3584 14336 28 128 0.99\nTable A4jFLOP comparison. For a variety of di\ufb00erent model sizes, we show the ratio of the FLOPs\nthat we compute per sequence to that using the 6\ud835\udc41\ud835\udc37approximation.\naccurate calculation giving a slightly di\ufb00erent value ( 6\u00933\u00021023compared to 5\u009376\u00021023).\n28", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d2dfcc9-a505-4d2e-9503-80ee8d75615d": {"__data__": {"id_": "2d2dfcc9-a505-4d2e-9503-80ee8d75615d", "embedding": null, "metadata": {"page_label": "29", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40364f47-f234-4556-ab03-c812ef5fa704", "node_type": "4", "metadata": {"page_label": "29", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "47c9a769ba63e92603b9d7f867808517d2f18b5e7ee0e96df29388eb48336951", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a72e312-5571-4322-b5a9-979f57fa83de", "node_type": "1", "metadata": {"page_label": "28", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "560a8cbe099af2a5c10cdb1878751a270194d28c7d22bf1219c5cbaf93c398bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "513dbdb1-d10a-499a-a1a7-2a78c6fef113", "node_type": "1", "metadata": {"page_label": "30", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "94352a6f919fc15a3b89863f4a6cfeb133a517a8f4af6d343188cc340c16feb0", "class_name": "RelatedNodeInfo"}}, "hash": "8fb83cc173b6e082a597c7a43a28073eb56aa816e1fcf7ee1a79e3da9af09e38", "text": "G. Other di\ufb00erences between Chinchilla andGopher\nBeyond di\ufb00erences in model size and number of training tokens, there are some additional minor\ndi\ufb00erences between Chinchilla andGopher. Speci\ufb01cally, Gopherwas trained with Adam (Kingma and\nBa, 2014) whereas Chinchilla was trained with AdamW (Loshchilov and Hutter, 2019). Furthermore,\nas discussed in Lessons Learned in Rae et al. (2021), Chinchilla stored a higher-precision copy of the\nweights in the sharded optimiser state.\nWe show comparisons of models trained with Adam and AdamW in Figure A6 and Figure A7.\nWe \ufb01nd that, independent of the learning rate schedule, AdamW trained models outperform models\ntrained with Adam. In Figure A6 we show a comparison of an 680 million parameter model trained\n0 5 10 15 20 25 30\nMillion Sequences2.452.502.552.602.652.70Training Loss\n0 5 10 15 20 25 30\nMillion Sequences17181920212223242526Wikitext103 Perplexity\n0 5 10 15 20 25 30\nMillion Sequences2.602.652.702.752.802.852.902.953.00C4 LossTraining Setup\nAdam w/ High Precision\nAdamW w/ High Precision\nAdam No High Precision\nAdamW No High Precision\nFigure A6jComparison of other di\ufb00erences. Using an 680 million parameter model, we show a\ncomparison between the setup used to train GopherandChinchilla \u2014 the change in optimiser and\nusing a higher precision copy of the weights in the optimiser state. The setup used for Chinchilla\n(orange) clearly outperforms the setup used to train Gopher(green).\n0 25 50 75 100 125 150\nMillion Sequences2.32.42.52.62.72.8C4 Loss\n0 25 50 75 100 125 150\nMillion Sequences10.012.515.017.520.022.525.027.530.0Wikitext103 Perplexity\n0 25 50 75 100 125 150\nMillion Sequences0.00.10.20.30.40.50.6LAMBADA Accuracy417M, Adam\n417M, AdamW\n1.4B, Adam\n1.4B, AdamW\nFigure A7jAdam vs AdamW. For a 417M (blue) and 1.4B model (green), we \ufb01nd that training with\nAdamW improves performance over training with Adam.\nwith and without the higher precision copy of the weights and with Adam/AdamW for comparison.\nH. Results\nH.1. The Pile\nIn Table A5 we show the bits-per-byte (bpb) on The Pile (Gao et al., 2020) of Chinchilla ,Gopher,\nand Jurassic-1. Chinchilla outperforms Gopheron all subsets. Jurassic-1 outperforms Chinchilla on 2\nsubsets\u2014 dm_mathematics andubuntu_irc .\n29", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "513dbdb1-d10a-499a-a1a7-2a78c6fef113": {"__data__": {"id_": "513dbdb1-d10a-499a-a1a7-2a78c6fef113", "embedding": null, "metadata": {"page_label": "30", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6a38b887-6a27-4fdb-9462-ad31875cc8c0", "node_type": "4", "metadata": {"page_label": "30", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "61f33dce84ec8586910f91c90fc9e901b14be8a4dad4f6069048bcdaf6a4fc87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d2dfcc9-a505-4d2e-9503-80ee8d75615d", "node_type": "1", "metadata": {"page_label": "29", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "8fb83cc173b6e082a597c7a43a28073eb56aa816e1fcf7ee1a79e3da9af09e38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2e1306b-c5b1-484c-a3bf-5b63f80b5525", "node_type": "1", "metadata": {"page_label": "31", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "807ec7fb6da2f068ad2f73087cdc98dc7c29b89c7daab44d72eaafadd31c7cb8", "class_name": "RelatedNodeInfo"}}, "hash": "94352a6f919fc15a3b89863f4a6cfeb133a517a8f4af6d343188cc340c16feb0", "text": "Subset Chinchilla (70B)Gopher(280B) Jurassic-1 (170B)\npile_cc 0.667 0.691 0.669\npubmed_abstracts 0.559 0.578 0.587\nstackexchange 0.614 0.641 0.655\ngithub 0.337 0.377 0.358\nopenwebtext2 0.647 0.677 -\narxiv 0.627 0.662 0.680\nuspto_backgrounds 0.526 0.546 0.537\nfreelaw 0.476 0.513 0.514\npubmed_central 0.504 0.525 0.579\ndm_mathematics 1.111 1.142 1.037\nhackernews 0.859 0.890 0.869\nnih_exporter 0.572 0.590 0.590\nopensubtitles 0.871 0.900 0.879\neuroparl 0.833 0.938 -\nbooks3 0.675 0.712 0.835\nphilpapers 0.656 0.695 0.742\ngutenberg_pg_19 0.548 0.656 0.890\nbookcorpus2 0.714 0.741 -\nubuntu_irc 1.026 1.090 0.857\nTableA5jBits-per-ByteonThePile. WeshowthebpbonThePilefor Chinchilla comparedto Gopher\nand Jurassic-1.\nH.2. MMLU\nIn Table A6 we show the performance of Chinchilla andGopheron each subset of MMLU.\nH.3. Winogender Setup\nWe follow the same setup as in Rae et al. (2021). To test coreference resolution in Chinchilla , we\ninput a sentence which includes a pronoun reference (e.g., \u201cThe librarian helped the child pick out a\nbook because {pronoun} liked to encourage reading.\u201d), then measure the probability of the model\ncompleting the sentence \u201c\u2018{Pronoun}\u2019 refers to the\u201d with di\ufb00erent sentence roles (\u201clibrarian\u201d and\n\u201cchild\u201din thisexample). Eachexampleisannotatedwiththe correctpronounresolution(thepronoun\ncorresponds to the librarian in this example). Each sentence is tested with a female, male, and\ngender-neutral pronoun. An unbiased model would correctly predict which word the pronoun refers\nto regardless of pronoun gender.\nH.4. BIG-bench\nInTableA7weshow Chinchilla andGopherperformanceoneachsubsetofBIG-benchthatweconsider.\nI. Model Card\nWe present the Chinchilla model card in Table A8, following the framework presented by Mitchell\net al. (2019).\n30", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2e1306b-c5b1-484c-a3bf-5b63f80b5525": {"__data__": {"id_": "f2e1306b-c5b1-484c-a3bf-5b63f80b5525", "embedding": null, "metadata": {"page_label": "31", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3aabd79-dc1d-4bc1-a2c2-3dc526c6ec10", "node_type": "4", "metadata": {"page_label": "31", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "618c15b4fb2a496cf17da3f4f141858459564b4be6cf4adf4346d3fc018df1fb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "513dbdb1-d10a-499a-a1a7-2a78c6fef113", "node_type": "1", "metadata": {"page_label": "30", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "94352a6f919fc15a3b89863f4a6cfeb133a517a8f4af6d343188cc340c16feb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a858936b-9acf-4c3c-bd1f-2c635f7907ce", "node_type": "1", "metadata": {"page_label": "32", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "775521cc36345634822c011625319743d5b9c8701f1daa8f7e41e0d7faef8ec4", "class_name": "RelatedNodeInfo"}}, "hash": "807ec7fb6da2f068ad2f73087cdc98dc7c29b89c7daab44d72eaafadd31c7cb8", "text": "Task Chinchilla Gopher Task Chinchilla Gopher\nabstract_algebra 31.0 25.0 anatomy 70.4 56.3\nastronomy 73.0 65.8 business_ethics 72.0 70.0\nclinical_knowledge 75.1 67.2 college_biology 79.9 70.8\ncollege_chemistry 51.0 45.0 college_computer_science 51.0 49.0\ncollege_mathematics 32.0 37.0 college_medicine 66.5 60.1\ncollege_physics 46.1 34.3 computer_security 76.0 65.0\nconceptual_physics 67.2 49.4 econometrics 38.6 43.0\nelectrical_engineering 62.1 60.0 elementary_mathematics 41.5 33.6\nformal_logic 33.3 35.7 global_facts 39.0 38.0\nhigh_school_biology 80.3 71.3 high_school_chemistry 58.1 47.8\nhigh_school_computer_science 58.0 54.0 high_school_european_history 78.8 72.1\nhigh_school_geography 86.4 76.8 high_school_gov_and_politics 91.2 83.9\nhigh_school_macroeconomics 70.5 65.1 high_school_mathematics 31.9 23.7\nhigh_school_microeconomics 77.7 66.4 high_school_physics 36.4 33.8\nhigh_school_psychology 86.6 81.8 high_school_statistics 58.8 50.0\nhigh_school_us_history 83.3 78.9 high_school_world_history 85.2 75.1\nhuman_aging 77.6 66.4 human_sexuality 86.3 67.2\ninternational_law 90.9 77.7 jurisprudence 79.6 71.3\nlogical_fallacies 80.4 72.4 machine_learning 41.1 41.1\nmanagement 82.5 77.7 marketing 89.7 83.3\nmedical_genetics 69.0 69.0 miscellaneous 84.5 75.7\nmoral_disputes 77.5 66.8 moral_scenarios 36.5 40.2\nnutrition 77.1 69.9 philosophy 79.4 68.8\nprehistory 81.2 67.6 professional_accounting 52.1 44.3\nprofessional_law 56.5 44.5 professional_medicine 75.4 64.0\nprofessional_psychology 75.7 68.1 public_relations 73.6 71.8\nsecurity_studies 75.9 64.9 sociology 91.0 84.1\nus_foreign_policy 92.0 81.0 virology 53.6 47.0\nworld_religions 87.7 84.2\nTable A6jChinchilla MMLU results. For each subset of MMLU (Hendrycks et al., 2020), we show\nChinchilla \u2019s accuracy compared to Gopher.\nModel Details\nOrganizationDevelopingtheModel DeepMind\nModel Date March 2022\nModel Type AutoregressiveTransformerLanguageModel(Section4.1for\ndetails)\nFeedback on the Model {jordanhoffmann, sborgeaud,\namensch,sifre}@deepmind.com\nIntended Uses\nPrimary Intended Uses The primary use is research on language models, including:\nresearch on the scaling behaviour of language models along\nwith those listed in Rae et al. (2021).\n31", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a858936b-9acf-4c3c-bd1f-2c635f7907ce": {"__data__": {"id_": "a858936b-9acf-4c3c-bd1f-2c635f7907ce", "embedding": null, "metadata": {"page_label": "32", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da77baf-cafc-4db0-b96d-0fb377dcb6ae", "node_type": "4", "metadata": {"page_label": "32", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "9ef2d4d06bba8a90f62336c4481ca93cf93857651d8d573dd2a8f24e875ab702", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2e1306b-c5b1-484c-a3bf-5b63f80b5525", "node_type": "1", "metadata": {"page_label": "31", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "807ec7fb6da2f068ad2f73087cdc98dc7c29b89c7daab44d72eaafadd31c7cb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4430a9e9-56b2-416f-ab75-56a800f803ef", "node_type": "1", "metadata": {"page_label": "33", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "ad1e7e71fbbc325205b16a14f05f363b840c8b255cdba12d1a31fe00a25d240a", "class_name": "RelatedNodeInfo"}}, "hash": "775521cc36345634822c011625319743d5b9c8701f1daa8f7e41e0d7faef8ec4", "text": "Primary Intended Users DeepMindresearchers. Wewillnotmakethismodelavailable\npublicly.\nOut-of-Scope Uses Uses of the language model for language generation in harm-\nfulordeceitfulsettings. Moregenerally,themodelshouldnot\nbe used for downstream applications without further safety\nand fairness mitigations.\nFactors\nCard Prompts \u2013 Relevant Factor Relevantfactorsincludewhichlanguageisused. Ourmodelis\ntrained on English data. Furthermore, in the analysis of mod-\nelstrainedonthesamecorpusinRaeetal.(2021),wefound\nit has unequal performance when modelling some dialects\n(e.g., African American English). Our model is designed for\nresearch. The model should not be used for downstream ap-\nplications without further analysis on factors in the proposed\ndownstream application.\nCard Prompts \u2013 Evaluation Factors See the results in Rae et al. (2021) which analyzes models\ntrained on the same text corpus.\nMetrics\nModel Performance Measures\n\u2022Perplexity and bits per byte on language modelling\ndatasets\n\u2022Accuracy on completion tasks, reading comprehension,\nMMLU, BIG-bench and fact checking.\n\u2022Exact match accuracy for question answering.\n\u2022Generation toxicity from Real Toxicity Prompts (RTP)\nalongside toxicity classi\ufb01cation accuracy.\n\u2022Gender and occupation bias. Test include comparing\nthe probability of generating di\ufb00erent gender terms\nand the Winogender coreference resolution task.\nWe principally focus on Chinchilla \u2019s performance compared\ntoGopheron text likelihood prediction.\nDecision thresholds N/A\nApproachestoUncertaintyandVari-\nabilityDue to the costs of training large language models, we did\nnot train Chinchilla multiple times. However, the breadth\nof our evaluation on a range of di\ufb00erent task types gives a\nreasonable estimate of the overall performance of the model.\nFurthermore, the existence of another large model trained\non the same dataset ( Gopher) provides a clear point of com-\nparison.\nEvaluation Data\n32", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4430a9e9-56b2-416f-ab75-56a800f803ef": {"__data__": {"id_": "4430a9e9-56b2-416f-ab75-56a800f803ef", "embedding": null, "metadata": {"page_label": "33", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "490140c2-92ae-4ebc-8d1d-9ecf9bfa930d", "node_type": "4", "metadata": {"page_label": "33", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "7d5a0be3d0aa64744e26d5fcab7daf12ef47f81d727a6a6fe599755a14660a0c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a858936b-9acf-4c3c-bd1f-2c635f7907ce", "node_type": "1", "metadata": {"page_label": "32", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "775521cc36345634822c011625319743d5b9c8701f1daa8f7e41e0d7faef8ec4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1b44fb9-f03d-4fc7-a90f-2f3e465bde5a", "node_type": "1", "metadata": {"page_label": "34", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "540d6a4ca7bd92401b53bdf7cf3318d44b30731a77b2eb43866248140b387df1", "class_name": "RelatedNodeInfo"}}, "hash": "ad1e7e71fbbc325205b16a14f05f363b840c8b255cdba12d1a31fe00a25d240a", "text": "Datasets\n\u2022Language modelling on LAMBADA, Wikitext103 (Mer-\nity et al., 2017), C4 (Ra\ufb00el et al., 2020a), PG-19 (Rae\net al., 2020) and the Pile (Gao et al., 2020).\n\u2022Language understanding, real world knowledge,\nmathematical and logical reasoning on the Massive\nMultitask Language Understanding (MMLU) bench-\nmark (Hendrycks et al., 2020) and on the \u201cBeyond the\nImitation Game Benchmark\u201d (BIG-bench) (BIG-bench\ncollaboration, 2021).\n\u2022Question answering (closed book) on Natural Ques-\ntions (Kwiatkowski et al., 2019) and TriviaQA (Joshi\net al., 2017).\n\u2022Reading comprehension on RACE (Lai et al., 2017)\n\u2022Common sense understanding on HellaSwag (Zellers\net al., 2019), PIQA (Bisk et al., 2020), Wino-\ngrande(Sakaguchietal.,2020),SIQA(Sapetal.,2019),\nBoolQ (Clark et al., 2019), and TruthfulQA (Lin et al.,\n2021).\nMotivation We chose evaluations from Rae et al. (2021) to allow us to\nmost directly compare to Gopher.\nPreprocessing Input text is tokenized using a SentencePiece tokenizer with\na vocabulary of size 32,000. Unlike the tokenizer used for\nGopher, the tokenizer used for Chinchilla does not perform\nNFKC normalization.\nTraining Data\nThe same dataset is used as in Rae et al. (2021). Di\ufb00erences in sampling are shown in Table A1.\nQuantitative Analyses\nUnitary Results Section 4.2 gives a detailed description of our analysis. Main\ntake-aways include:\n\u2022Our model is capable of outputting toxic language as\nmeasured by the PerspectiveAPI. This is particularly\ntrue when the model is prompted with toxic prompts.\n\u2022Gender: Our model emulates stereotypes found in our\ndataset, with occupations such as \u201cdietician\u201d and \u201cre-\nceptionist\u201dbeingmoreassociatedwithwomenand\u201ccar-\npenter\u201d and \u201csheri\ufb00\u201d being more associated with men.\n\u2022Race/religion/country sentiment: Prompting our\nmodel to discuss some groups leads to sentences with\nlower or higher sentiment, likely re\ufb02ecting text in our\ndataset.\n33", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1b44fb9-f03d-4fc7-a90f-2f3e465bde5a": {"__data__": {"id_": "e1b44fb9-f03d-4fc7-a90f-2f3e465bde5a", "embedding": null, "metadata": {"page_label": "34", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8258fff-dc68-4d2b-8de8-03f922fd5aea", "node_type": "4", "metadata": {"page_label": "34", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "94130e12606285a9f0f6e7a95ed0f6f531f8b9c0183c9881a37ef08fbf83c81c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4430a9e9-56b2-416f-ab75-56a800f803ef", "node_type": "1", "metadata": {"page_label": "33", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "ad1e7e71fbbc325205b16a14f05f363b840c8b255cdba12d1a31fe00a25d240a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c4b5a02-82a9-42a3-b653-a9cc1c608bfb", "node_type": "1", "metadata": {"page_label": "35", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "d99e7951d41be6f8f90ae992eaa8984d7c68dc2d1423429c3e396ee95d52eba6", "class_name": "RelatedNodeInfo"}}, "hash": "540d6a4ca7bd92401b53bdf7cf3318d44b30731a77b2eb43866248140b387df1", "text": "Intersectional Results We did not investigate intersectional biases.\nEthical Considerations\nData The data is the same as described in Rae et al. (2021).\nHuman Life The model is not intended to inform decisions about matters\ncentral to human life or \ufb02ourishing.\nMitigations We considered \ufb01ltering the dataset to remove toxic content\nbut decided against it due to the observation that this can\nintroduce new biases as studied by Welbl et al. (2021). More\nworkisneededonmitigationapproachestotoxiccontentand\nother types of risks associated with language models, such\nas those discussed in Weidinger et al. (2021).\nRisks and Harms Thedataiscollectedfromtheinternet,andthusundoubtedly\nthere is toxic/biased content in our training dataset. Fur-\nthermore, it is likely that personal information is also in the\ndataset that has been used to train our models. We defer to\nthe more detailed discussion in Weidinger et al. (2021).\nUse Cases Especially fraught use cases include the generation of fac-\ntually incorrect information with the intent of distributing\nit or using the model to generate racist, sexist or otherwise\ntoxic text with harmful intent. Many more use cases that\ncould cause harm exist. Such applications to malicious use\nare discussed in detail in Weidinger et al. (2021).\nTable A8jChinchilla model card. We follow the framework presented in Mitchell et al. (2019).\nJ. List of trained models\nIn Table A9 we list the model size and con\ufb01guration of all models used in this study. Many models\nhave been trained multiple times, for a di\ufb00erent number of training steps.\n34", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c4b5a02-82a9-42a3-b653-a9cc1c608bfb": {"__data__": {"id_": "1c4b5a02-82a9-42a3-b653-a9cc1c608bfb", "embedding": null, "metadata": {"page_label": "35", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77e9d094-8f80-42fe-b28b-53739391062c", "node_type": "4", "metadata": {"page_label": "35", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "df39ce466a0e41ab537453af6538a2eb49fd6836ff88fa95ef03f1d2c902b80a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1b44fb9-f03d-4fc7-a90f-2f3e465bde5a", "node_type": "1", "metadata": {"page_label": "34", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "540d6a4ca7bd92401b53bdf7cf3318d44b30731a77b2eb43866248140b387df1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60991f2d-f3a6-4739-a4d5-d4574b79afc4", "node_type": "1", "metadata": {"page_label": "36", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "50253bf681a0ef3ae6decfacfcae7fad730b0ca5dd07e46c8f5088fb8df87028", "class_name": "RelatedNodeInfo"}}, "hash": "d99e7951d41be6f8f90ae992eaa8984d7c68dc2d1423429c3e396ee95d52eba6", "text": "Task Chinchilla Gopher Task Chinchilla Gopher\nhyperbaton 54.2 51.7 movie_dialog_same_or_di\ufb00 54.5 50.7\ncausal_judgment 57.4 50.8 winowhy 62.5 56.7\nformal_fallacies_syllogisms_neg 52.1 50.7 movie_recommendation 75.6 50.5\ncrash_blossom 47.6 63.6 moral_permissibility 57.3 55.1\ndiscourse_marker_prediction 13.1 11.7 strategyqa 68.3 61.0\ngeneral_knowledge_json 94.3 93.9 nonsense_words_grammar 78.0 61.4\nsports_understanding 71.0 54.9 metaphor_boolean 93.1 59.3\nimplicit_relations 49.4 36.4 navigate 52.6 51.1\npenguins_in_a_table 48.7 40.6 presuppositions_as_nli 49.9 34.0\nintent_recognition 92.8 88.7 temporal_sequences 32.0 19.0\nreasoning_about_colored_objects 59.7 49.2 question_selection 52.6 41.4\nlogic_grid_puzzle 44.0 35.1 logical_fallacy_detection 72.1 58.9\ntimedial 68.8 50.9 physical_intuition 79.0 59.7\nepistemic_reasoning 60.6 56.4 physics_mc 65.5 50.9\nruin_names 47.1 38.6 identify_odd_metaphor 68.8 38.6\nhindu_knowledge 91.4 80.0 understanding_fables 60.3 39.6\nmisconceptions 65.3 61.7 logical_sequence 64.1 36.4\nimplicatures 75.0 62.0 mathematical_induction 47.3 57.6\ndisambiguation_q 54.7 45.5 fantasy_reasoning 69.0 64.1\nknown_unknowns 65.2 63.6 SNARKS 58.6 48.3\ndark_humor_detection 66.2 83.1 crass_ai 75.0 56.8\nanalogical_similarity 38.1 17.2 entailed_polarity 94.0 89.5\nsentence_ambiguity 71.7 69.1 irony_identi\ufb01cation 73.0 69.7\nriddle_sense 85.7 68.2 evaluating_info_essentiality 17.6 16.7\ndate_understanding 52.3 44.1 phrase_relatedness 94.0 81.8\nanalytic_entailment 67.1 53.0 novel_concepts 65.6 59.1\nodd_one_out 70.9 32.5 empirical_judgments 67.7 52.5\nlogical_args 56.2 59.1 \ufb01gure_of_speech_detection 63.3 52.7\nalignment_questionnaire 91.3 79.2 english_proverbs 82.4 57.6\nsimilarities_abstraction 87.0 81.8 Human_organs_senses_mcc 85.7 84.8\nanachronisms 69.1 56.4 gre_reading_comprehension 53.1 27.3\nTable A7jChinchilla BIG-bench results. For each subset of BIG-bench (BIG-bench collaboration,\n2021), we show Chinchilla andGopher\u2019s accuracy.\n35", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60991f2d-f3a6-4739-a4d5-d4574b79afc4": {"__data__": {"id_": "60991f2d-f3a6-4739-a4d5-d4574b79afc4", "embedding": null, "metadata": {"page_label": "36", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd134f4-e9ee-41b4-8db5-d4466a56bc56", "node_type": "4", "metadata": {"page_label": "36", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "d4113fb81a8cc7c62625e027d6bfa10cca46710fe1f42932ad3be4e5c2ab8b9e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c4b5a02-82a9-42a3-b653-a9cc1c608bfb", "node_type": "1", "metadata": {"page_label": "35", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}, "hash": "d99e7951d41be6f8f90ae992eaa8984d7c68dc2d1423429c3e396ee95d52eba6", "class_name": "RelatedNodeInfo"}}, "hash": "50253bf681a0ef3ae6decfacfcae7fad730b0ca5dd07e46c8f5088fb8df87028", "text": "Parameters (million) d_model \ufb00w_size kv_size n_heads n_layers\n44 512 2048 64 8 8\n57 576 2304 64 9 9\n74 640 2560 64 10 10\n90 640 2560 64 10 13\n106 640 2560 64 10 16\n117 768 3072 64 12 12\n140 768 3072 64 12 15\n163 768 3072 64 12 18\n175 896 3584 64 14 14\n196 896 3584 64 14 16\n217 896 3584 64 14 18\n251 1024 4096 64 16 16\n278 1024 4096 64 16 18\n306 1024 4096 64 16 20\n425 1280 5120 128 10 18\n489 1280 5120 128 10 21\n509 1408 5632 128 11 18\n552 1280 5120 128 10 24\n587 1408 5632 128 11 21\n632 1536 6144 128 12 19\n664 1408 5632 128 11 24\n724 1536 6144 128 12 22\n816 1536 6144 128 12 25\n893 1792 7168 128 14 20\n1,018 1792 7168 128 14 23\n1,143 1792 7168 128 14 26\n1,266 2048 8192 128 16 22\n1,424 2176 8704 128 17 22\n1,429 2048 8192 128 16 25\n1,593 2048 8192 128 16 28\n1,609 2176 8704 128 17 25\n1,731 2304 9216 128 18 24\n1,794 2176 8704 128 17 28\n2,007 2304 9216 128 18 28\n2,283 2304 9216 128 18 32\n2,298 2560 10240 128 20 26\n2,639 2560 10240 128 20 30\n2,980 2560 10240 128 20 34\n3,530 2688 10752 128 22 36\n3,802 2816 11264 128 22 36\n4,084 2944 11776 128 22 36\n4,516 3072 12288 128 24 36\n6,796 3584 14336 128 28 40\n9,293 4096 16384 128 32 42\n11,452 4352 17408 128 32 47\n12,295 4608 18432 128 36 44\n12,569 4608 18432 128 32 47\n13,735 4864 19456 128 32 47\n14,940 4992 19968 128 32 49\n16,183 5120 20480 128 40 47\nTable A9jAll models. We list the hyperparameters and size of all models trained as part of this work.\nMany shown models have been trained with multiple learning rate schedules/number of training\ntokens.\n36", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"e96f6f8b-7d82-445c-8f69-3709abfde183": {"node_ids": ["b9cad8a4-43d4-4d9e-8a63-8bdab5ef612a"], "metadata": {"page_label": "1", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "e0ec18af-7044-4d71-8e1c-0c57075d1b0d": {"node_ids": ["288068d9-ccea-44c3-b8d5-3d53f7c0d53c"], "metadata": {"page_label": "2", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "a40b57fa-e19c-4778-8152-de96ea340d2c": {"node_ids": ["493a97bc-d778-42a9-8746-544b4d50a35b"], "metadata": {"page_label": "3", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "a92888dd-3cf4-4d1b-85cc-8d69fb25ec66": {"node_ids": ["e6556128-f1d0-46fc-9ca4-bf0d7c971fab"], "metadata": {"page_label": "4", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "f1fb6a8a-bffb-4f5d-8a41-18b4789ed1e8": {"node_ids": ["618c011b-1ce5-4e27-a5fc-2455a2e03ba0"], "metadata": {"page_label": "5", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "53ece806-c0ac-496d-b0c1-aba2fd9d5584": {"node_ids": ["9d18bff4-9186-4099-9952-58b5551259b8"], "metadata": {"page_label": "6", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "03090305-db0a-49a6-8abd-fc340693a1db": {"node_ids": ["e56df99d-4c1e-4a22-b5ba-4ec6710c59e0"], "metadata": {"page_label": "7", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "fea356c1-9131-4f17-bd90-2bc2553ff672": {"node_ids": ["a448586b-c9af-4e96-9f15-ff34dfeb5847"], "metadata": {"page_label": "8", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "727c9798-98de-46db-a290-c0d01aca189f": {"node_ids": ["dff7ed46-7222-48ae-8dbc-883f3fa40d4c"], "metadata": {"page_label": "9", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "70018603-c48e-4570-9b60-3a7346ec33df": {"node_ids": ["cadd7b48-c685-4b46-b459-4836ba6a8abc"], "metadata": {"page_label": "10", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "d4ccbabb-52f3-4580-a827-e608498351e5": {"node_ids": ["4767d97b-867e-4955-a481-8323277bf8d3"], "metadata": {"page_label": "11", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "3f493e24-fb48-404d-bce6-93f9ccd989ad": {"node_ids": ["80758997-9f1b-4bca-82c9-03e6b2ee7573"], "metadata": {"page_label": "12", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "cbde5101-5369-4482-865c-46111003cf86": {"node_ids": ["11c93f64-82f8-493a-8629-50b50b9d3418"], "metadata": {"page_label": "13", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "756df0d3-df4b-411f-94e3-1af878c90ae9": {"node_ids": ["28a447d6-1557-4e5e-ab15-c95c313875f9"], "metadata": {"page_label": "14", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "9f751dea-9c23-4922-9deb-1b796b769609": {"node_ids": ["58a94e53-e6c2-4ce0-bc16-67e614f0dd78"], "metadata": {"page_label": "15", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "7cac0a20-b98d-4376-a7a2-eecc378e53c3": {"node_ids": ["91b665e9-1cac-4ff9-8338-27e2b3d9e9bd"], "metadata": {"page_label": "16", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "337dc3d9-dc1c-4087-b50e-bed6e0fab618": {"node_ids": ["6a5b5df5-5f3f-4826-bd06-ab41668e9cf1", "f14cab1b-e278-4979-814d-3327508f1542"], "metadata": {"page_label": "17", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "1ac69fbe-54d7-465c-92d2-fd2fee280ce3": {"node_ids": ["4c23b670-23a4-484c-9698-e366e6f68ed1", "c1fb69bf-82ff-4593-84e2-7b6a5545408c"], "metadata": {"page_label": "18", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "1815f9be-960c-48ea-8f5c-790013fc6516": {"node_ids": ["2977affb-138a-44d2-8c73-23ba3fdd1a25", "6cd3667d-de22-4b44-9159-99a8f73712de"], "metadata": {"page_label": "19", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "b9f11827-92a1-4bf9-8481-9bb2e8276320": {"node_ids": ["afa700a0-cc75-4678-b80d-d3a76c40a78a", "bb2b22ac-d07c-4a1e-aabd-d412050a6f46"], "metadata": {"page_label": "20", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "8d8af2b0-6414-40a6-b5a5-0d5bc422e002": {"node_ids": ["8d2c7fee-73d6-4fd9-9d31-e35c1116bf74"], "metadata": {"page_label": "21", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "73cd2ddd-601f-43cc-b123-6f3d58f21df9": {"node_ids": ["97c5a966-cf49-4ac8-be31-fb4a105f9472"], "metadata": {"page_label": "22", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "ae03e0cd-6484-4f01-8ba6-1e8ceebe5b09": {"node_ids": ["d37074ae-af0d-4880-872e-db300881db35"], "metadata": {"page_label": "23", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "f79e085a-e4ba-4c3e-8364-6d874eaa357a": {"node_ids": ["c01521d1-ae3f-4321-9d9f-1030f1080c2b"], "metadata": {"page_label": "24", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "06dde59c-1d62-4992-84f4-2d56e4115733": {"node_ids": ["84fa3ab5-e358-4ddb-ac8b-6953e7a3db47", "a2828e88-65e6-4051-96b1-8aee312e38aa"], "metadata": {"page_label": "25", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "75dd8a66-ad33-40a8-a872-06a9238ef1be": {"node_ids": ["3bf48011-656f-4851-aa36-f5eab3d37d77"], "metadata": {"page_label": "26", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "ce1291ec-6f1b-4c66-aa69-8728baa54296": {"node_ids": ["49c96674-a0d5-49a8-b790-b39101aa00cb"], "metadata": {"page_label": "27", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "16287717-1851-44fb-9b74-fd4e34938784": {"node_ids": ["6a72e312-5571-4322-b5a9-979f57fa83de"], "metadata": {"page_label": "28", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "40364f47-f234-4556-ab03-c812ef5fa704": {"node_ids": ["2d2dfcc9-a505-4d2e-9503-80ee8d75615d"], "metadata": {"page_label": "29", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "6a38b887-6a27-4fdb-9462-ad31875cc8c0": {"node_ids": ["513dbdb1-d10a-499a-a1a7-2a78c6fef113"], "metadata": {"page_label": "30", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "f3aabd79-dc1d-4bc1-a2c2-3dc526c6ec10": {"node_ids": ["f2e1306b-c5b1-484c-a3bf-5b63f80b5525"], "metadata": {"page_label": "31", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "1da77baf-cafc-4db0-b96d-0fb377dcb6ae": {"node_ids": ["a858936b-9acf-4c3c-bd1f-2c635f7907ce"], "metadata": {"page_label": "32", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "490140c2-92ae-4ebc-8d1d-9ecf9bfa930d": {"node_ids": ["4430a9e9-56b2-416f-ab75-56a800f803ef"], "metadata": {"page_label": "33", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "e8258fff-dc68-4d2b-8de8-03f922fd5aea": {"node_ids": ["e1b44fb9-f03d-4fc7-a90f-2f3e465bde5a"], "metadata": {"page_label": "34", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "77e9d094-8f80-42fe-b28b-53739391062c": {"node_ids": ["1c4b5a02-82a9-42a3-b653-a9cc1c608bfb"], "metadata": {"page_label": "35", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}, "ecd134f4-e9ee-41b4-8db5-d4466a56bc56": {"node_ids": ["60991f2d-f3a6-4739-a4d5-d4574b79afc4"], "metadata": {"page_label": "36", "file_name": "2203.15556.pdf", "file_path": "data\\2203.15556.pdf", "file_type": "application/pdf", "file_size": 6004349, "creation_date": "2023-11-29", "last_modified_date": "2023-11-29", "last_accessed_date": "2023-11-29"}}}}